{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNkyZE04XRt/MbF1vOMQkJ5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ram130849/Deep_Learning_Systems_Assignments/blob/main/PyTorch/Nithin/DLS_Assignment_1_nvaradha.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Learning Systems Fall 2022 Assignment 1\n",
        "\n",
        "### Nithin Varadharajan (nvaradha)"
      ],
      "metadata": {
        "id": "IyKoaNOJYcT0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 1: A Detailed View to MNIST Classification"
      ],
      "metadata": {
        "id": "4YX2ccSxZEhW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Train a fully-connected net for MNIST classification (sorry, no CNN please, yet). It should\n",
        "be with 5 hidden layers each of which is with 1024 hidden units. Feel free to use whatever\n",
        "techniques you learned in class. You should be able to get the test accuracy above 98%.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Z1HrLW7_fNl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "EVoYmKHJf2K3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "h0qzvhHHIYqv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDaSPYi2JYnm",
        "outputId": "c99eb237-f43a-4e69-a019-589bca9d4155"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda:0 device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "mnist_train=torchvision.datasets.MNIST('mnist',\n",
        "                                      train=True,\n",
        "                                      download=True,\n",
        "                                      transform=torchvision.transforms.Compose([\n",
        "                                                                                torchvision.transforms.ToTensor(),\n",
        "                                                                                torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                                                                ]))\n",
        "\n",
        "mnist_test=torchvision.datasets.MNIST('mnist',\n",
        "                                      train=False,\n",
        "                                      download=True,\n",
        "                                      transform=torchvision.transforms.Compose([\n",
        "                                                                                torchvision.transforms.ToTensor(),\n",
        "                                                                                torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                                                                ]))\n"
      ],
      "metadata": {
        "id": "t7VgwdEOg-bg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# some hyperparameters\n",
        "n_epochs = 50\n",
        "batch_size_train =7000\n",
        "batch_size_test = 1000\n",
        "\n",
        "random_seed = 1\n",
        "torch.backends.cudnn.enabled = False\n",
        "torch.manual_seed(random_seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMTUfo8BhsBV",
        "outputId": "895c1a0e-c9c1-45d8-b07a-9fa2cf1feeb3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f17d9aeb710>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Reading number of samples in each set\n",
        "n_train = len(mnist_train)\n",
        "n_test = len(mnist_test)\n",
        "\n",
        "print(n_train)\n",
        "print(n_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEV9eSzoJFsb",
        "outputId": "83b8a679-8e27-4e52-d9c4-f066b025292b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000\n",
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data loaders\n",
        "train_loader = torch.utils.data.DataLoader(mnist_train,batch_size=batch_size_train, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(mnist_test,batch_size=batch_size_test, shuffle=True)"
      ],
      "metadata": {
        "id": "7Xgd4LT-VvGG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the network\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "9l00qZqFXeHd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Inputs to hidden layer linear transformation\n",
        "        self.input = nn.Linear(784, 1024)\n",
        "        # Output layer, 10 units - one for each digit\n",
        "        self.fc1 = nn.Linear(1024, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 1024)\n",
        "        self.fc3 = nn.Linear(1024, 1024)\n",
        "        self.fc4 = nn.Linear(1024, 1024)\n",
        "        self.fc5 = nn.Linear(1024, 1024)\n",
        "        self.output = nn.Linear(1024, 10)\n",
        "        \n",
        "        # Define sigmoid activation and softmax output \n",
        "        self.ReLU = nn.ReLU()\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        #x = self.flatten(x)\n",
        "        #print(x.size())\n",
        "        # Pass the input tensor through each of our operations\n",
        "        x = self.input(x)\n",
        "        x = self.ReLU(x)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.ReLU(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.ReLU(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        x = self.ReLU(x)\n",
        "\n",
        "        x = self.fc4(x)\n",
        "        x = self.ReLU(x)\n",
        "\n",
        "        x = self.fc5(x)\n",
        "        x = self.ReLU(x)\n",
        "\n",
        "        x = self.output(x)\n",
        "        x = F.softmax(x, dim = 1)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dfUP3od_X0eX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Uniform initialization      \n",
        "def weights_init_uniform_rule(m):\n",
        "  classname = m.__class__.__name__\n",
        "  # for every Linear layer in a model..\n",
        "  if classname.find('Linear') != -1:\n",
        "      # get the number of the inputs\n",
        "      n = m.in_features\n",
        "      y = 1.0/np.sqrt(n)\n",
        "      m.weight.data.uniform_(-y, y)\n",
        "      m.bias.data.fill_(0)\n",
        "\n",
        "model = Network().to(device)\n",
        "model.apply(weights_init_uniform_rule)\n",
        "\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOOjixwM7F3E",
        "outputId": "c7273839-09ae-4517-f6ee-cdf189a0ed78"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network(\n",
            "  (input): Linear(in_features=784, out_features=1024, bias=True)\n",
            "  (fc1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc3): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc4): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc5): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (output): Linear(in_features=1024, out_features=10, bias=True)\n",
            "  (ReLU): ReLU()\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FTbixZCnLdoE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# send parameters to device for gpu\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "loss_func = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "I8sDaw9YgQ9O"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "train_counter = []\n",
        "test_losses = []\n",
        "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]\n"
      ],
      "metadata": {
        "id": "kHa7SzI-ieRm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "  train_loss= 0\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    images_reshaped = data.reshape(-1,784).to(device)\n",
        "    labels = target.to(device)\n",
        "\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(images_reshaped)\n",
        "    loss = loss_func(outputs, labels)\n",
        "\n",
        "    # Backward and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    # with torch.no_grad():\n",
        "    #   val_images,val_labels = next(iter(val_loader))\n",
        "    #   val_images_reshaped = val_images.reshape(-1, input_dm1*input_dm2).to(device)\n",
        "    #   val_labels_pred = model.forward(val_images_reshaped).argmax(1).cpu().numpy()\n",
        "    #   performance = eval_func(val_labels.numpy(),val_labels_pred)\n",
        "\n",
        "\n",
        "    #optimizer.zero_grad()\n",
        "  print(\"Average training loss per sample  = \"+str(train_loss/n_train))\n",
        "  train_loss = 0"
      ],
      "metadata": {
        "id": "5prL1KKQimvl"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "def test():\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      #output = model(data.to(device))\n",
        "      #test_loss += loss_func(output.to(device), target.to(device))\n",
        "   \n",
        "      #pred = (output.data.max(1, keepdim=True)[1]).cpu()\n",
        "      #correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "      \n",
        "      val_images_reshaped = data.reshape(-1, 784).to(device)\n",
        "      val_labels_pred = model.forward(val_images_reshaped).argmax(1).cpu().numpy()\n",
        "      performance = accuracy_score(target.numpy(),val_labels_pred)\n",
        "      print(performance)\n",
        "  #test_loss /= len(test_loader.dataset)\n",
        "  #test_losses.append(test_loss)\n",
        "  #print(performance)"
      ],
      "metadata": {
        "id": "69xedMhgiqt1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jvk08EV1fnva",
        "outputId": "00178569-5946-4f2b-c397-c8f065a5767c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.978\n",
            "0.968\n",
            "0.978\n",
            "0.981\n",
            "0.972\n",
            "0.979\n",
            "0.974\n",
            "0.972\n",
            "0.976\n",
            "0.979\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test()\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  train(epoch)\n",
        "  test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIM5T2x2iwJx",
        "outputId": "56671626-4f69-4375-bcf7-6c84aa0d29c9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.112\n",
            "Average training loss per sample  = 0.0003395279804865519\n",
            "0.342\n",
            "Average training loss per sample  = 0.0003020373543103536\n",
            "0.556\n",
            "Average training loss per sample  = 0.0002784874657789866\n",
            "0.633\n",
            "Average training loss per sample  = 0.0002671024481455485\n",
            "0.784\n",
            "Average training loss per sample  = 0.0002490809758504232\n",
            "0.876\n",
            "Average training loss per sample  = 0.00023654850721359252\n",
            "0.901\n",
            "Average training loss per sample  = 0.00023153042197227478\n",
            "0.924\n",
            "Average training loss per sample  = 0.00022921574314435322\n",
            "0.926\n",
            "Average training loss per sample  = 0.0002281187335650126\n",
            "0.938\n",
            "Average training loss per sample  = 0.00022701450983683268\n",
            "0.944\n",
            "Average training loss per sample  = 0.00022617329557736714\n",
            "0.961\n",
            "Average training loss per sample  = 0.00022556158105532329\n",
            "0.95\n",
            "Average training loss per sample  = 0.00022504740953445434\n",
            "0.964\n",
            "Average training loss per sample  = 0.00022437609831492107\n",
            "0.958\n",
            "Average training loss per sample  = 0.0002241708258787791\n",
            "0.968\n",
            "Average training loss per sample  = 0.0002235487163066864\n",
            "0.964\n",
            "Average training loss per sample  = 0.0002233876128991445\n",
            "0.977\n",
            "Average training loss per sample  = 0.00022306827704111735\n",
            "0.958\n",
            "Average training loss per sample  = 0.00022273992896080017\n",
            "0.968\n",
            "Average training loss per sample  = 0.00022254075606664022\n",
            "0.969\n",
            "Average training loss per sample  = 0.00022229671279589336\n",
            "0.973\n",
            "Average training loss per sample  = 0.00022243231932322184\n",
            "0.961\n",
            "Average training loss per sample  = 0.0002221441686153412\n",
            "0.974\n",
            "Average training loss per sample  = 0.00022196008960405984\n",
            "0.967\n",
            "Average training loss per sample  = 0.00022183266679445902\n",
            "0.962\n",
            "Average training loss per sample  = 0.00022162277897198995\n",
            "0.975\n",
            "Average training loss per sample  = 0.00022154788176218667\n",
            "0.978\n",
            "Average training loss per sample  = 0.00022147927482922872\n",
            "0.982\n",
            "Average training loss per sample  = 0.00022147146066029867\n",
            "0.972\n",
            "Average training loss per sample  = 0.00022135329643885294\n",
            "0.98\n",
            "Average training loss per sample  = 0.00022126757105191548\n",
            "0.977\n",
            "Average training loss per sample  = 0.00022110037207603454\n",
            "0.98\n",
            "Average training loss per sample  = 0.00022098428606987\n",
            "0.984\n",
            "Average training loss per sample  = 0.00022092198729515075\n",
            "0.985\n",
            "Average training loss per sample  = 0.00022099759976069132\n",
            "0.981\n",
            "Average training loss per sample  = 0.00022089906533559163\n",
            "0.968\n",
            "Average training loss per sample  = 0.00022080023090044658\n",
            "0.981\n",
            "Average training loss per sample  = 0.00022076463301976522\n",
            "0.972\n",
            "Average training loss per sample  = 0.00022078895966211955\n",
            "0.973\n",
            "Average training loss per sample  = 0.00022067541281382242\n",
            "0.984\n",
            "Average training loss per sample  = 0.0002206935783227285\n",
            "0.974\n",
            "Average training loss per sample  = 0.000220680832862854\n",
            "0.975\n",
            "Average training loss per sample  = 0.00022053767442703248\n",
            "0.977\n",
            "Average training loss per sample  = 0.0002205839474995931\n",
            "0.98\n",
            "Average training loss per sample  = 0.00022054385741551716\n",
            "0.976\n",
            "Average training loss per sample  = 0.00022060920000076294\n",
            "0.979\n",
            "Average training loss per sample  = 0.000220588219165802\n",
            "0.977\n",
            "Average training loss per sample  = 0.0002205077846844991\n",
            "0.969\n",
            "Average training loss per sample  = 0.0002204870839913686\n",
            "0.982\n",
            "Average training loss per sample  = 0.00022054651578267415\n",
            "0.978\n"
          ]
        }
      ]
    }
  ]
}