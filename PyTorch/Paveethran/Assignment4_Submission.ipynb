{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1iVXboD-6qYY",
    "outputId": "3841d3c9-1501-44b9-a990-2d773f7232bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: simplejson in /usr/local/lib/python3.7/dist-packages (3.18.0)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as func\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import SubsetRandomSampler, RandomSampler, random_split\n",
    "from torch.utils.data import TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import IPython\n",
    "import soundfile as sf\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import glob\n",
    "import os, random\n",
    "import pickle\n",
    "!pip install simplejson\n",
    "import simplejson\n",
    "import pandas as pd\n",
    "import ast\n",
    "from matplotlib.pyplot import figure\n",
    "import copy\n",
    "from itertools import combinations,product\n",
    "import seaborn as sns\n",
    "import gzip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0zcttZQI61H2",
    "outputId": "57fbeb53-8427-4377-c496-492717d46cc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZSLWdGzq635_"
   },
   "outputs": [],
   "source": [
    "folder_path = \"/content/drive/My Drive/Projects and research stuffs/DLS Assignments/Assignment 4/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "F-nQZeaB6_zg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lanENqxk7S1-"
   },
   "source": [
    "## Problem 1: Network compression using SVD - MNIST Classification with 6 D values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-WnOYTf7cnr"
   },
   "source": [
    "### Loading data and data loader for network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zmnkcVeM7diZ",
    "outputId": "672a5565-7f0e-4593-eb30-64fbd1eda9bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST Train shape torch.Size([60000, 28, 28]) <class 'torchvision.datasets.mnist.MNIST'>\n"
     ]
    }
   ],
   "source": [
    "mnist_train=torchvision.datasets.MNIST('mnist',\n",
    "                train=True,\n",
    "                download=True,\n",
    "                transform=torchvision.transforms.Compose([\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "                ]))\n",
    "\n",
    "mnist_test=torchvision.datasets.MNIST('mnist',\n",
    "                train=False,\n",
    "                download=True,\n",
    "                transform=torchvision.transforms.Compose([\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "                ]))\n",
    "\n",
    "print('MNIST Train shape', mnist_train.data.shape, type(mnist_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "E5BLZcGb7ndS"
   },
   "outputs": [],
   "source": [
    "batch_size_train = 128 # batch size train set\n",
    "batch_size_test = 128 # batch size test set\n",
    "\n",
    "# Dataloader for laoding in data\n",
    "# Steps to do this is to convert raw data to tensor\n",
    "# Then tensor to dataset object\n",
    "# And then from dataset object to dataloader\n",
    "\n",
    "# Our MNIST is already in tensorDataset format\n",
    "train_loader = torch.utils.data.DataLoader(dataset=mnist_train, \n",
    "                                             batch_size=batch_size_train, \n",
    "                                             shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=mnist_test, \n",
    "                                             batch_size=batch_size_test, \n",
    "                                             shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zS4gHr2c7-TT",
    "outputId": "d954da9c-2b95-401d-b83a-d2ed1662a960"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# Get device\n",
    "if(torch.cuda.is_available()):\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"Using {device} device\")\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "\n",
    "torch.backends.cudnn.enabled = False\n",
    "# Set torch random seed \n",
    "torch.manual_seed(100)\n",
    "np.random.seed(100)\n",
    "\n",
    "#Uniform initialization      \n",
    "def weights_init_uniform_rule(m):    \n",
    "    if isinstance(m, nn.Linear):\n",
    "        n = m.in_features # number of inputs\n",
    "        y = 1.0/np.sqrt(n)\n",
    "        m.weight.data.uniform_(-y, y)\n",
    "        m.bias.data.fill_(0)   \n",
    "        \n",
    "# Xavier initialization      \n",
    "def xavier_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0)\n",
    "    \n",
    "# Kaiming He initialization\n",
    "def he_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.kaiming_uniform_(m.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
    "        m.bias.data.fill_(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5AZ3Gin98XDq"
   },
   "source": [
    "### Define and run network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rm0eUlcPAAK-"
   },
   "source": [
    "#### Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "PIcjXw_r8OvO"
   },
   "outputs": [],
   "source": [
    "class baseline_q1(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fp_input = nn.Linear(28*28, 1024)\n",
    "        self.fp1 = nn.Linear(1024, 1024)\n",
    "        self.fp2 = nn.Linear(1024, 1024)\n",
    "        self.fp3 = nn.Linear(1024, 1024)\n",
    "        self.fp4 = nn.Linear(1024, 1024)\n",
    "        self.fp_output = nn.Linear(1024, 10)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.ReLU = nn.ReLU()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        to_save_layerop = {}\n",
    "        input = self.fp_input(input)\n",
    "        input = self.ReLU(input)\n",
    "        to_save_layerop['fp_input'] = input    \n",
    "        \n",
    "        input = self.fp1(input)\n",
    "        input = self.ReLU(input)\n",
    "        to_save_layerop['fp1'] = input\n",
    "        \n",
    "        input = self.fp2(input)\n",
    "        input = self.ReLU(input)\n",
    "        to_save_layerop['fp2'] = input\n",
    "        \n",
    "        input = self.fp3(input)\n",
    "        input = self.ReLU(input)\n",
    "        to_save_layerop['fp3'] = input\n",
    "        \n",
    "        input = self.fp4(input)\n",
    "        input = self.ReLU(input)\n",
    "        to_save_layerop['fp4'] = input\n",
    "\n",
    "        input = self.fp_output(input)\n",
    "        to_save_layerop['fp_output_before_softmax'] = input\n",
    "\n",
    "        # MOST IMPORTANT REALIZATION...USE SOFTMAX AFTER CALCULATING CCE LOSS, THE CCE LOSS EXPECTS LOGITS AND NOT SOFTMAX OUTPUTS.\n",
    "        return input, to_save_layerop    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "X-ndOuGh91lq"
   },
   "outputs": [],
   "source": [
    "def run_network(net, epochs, loss_criteria, optimizer, validation_process=True):\n",
    "    # A minor difference is that the implementation of CrossEntrypyLoss implicitly applies a softmax activation followed by a log transformation \n",
    "    # but NLLLoss does not.\n",
    "\n",
    "    # To keep best performance value\n",
    "    max_performance = float('-inf')\n",
    "    \n",
    "    # For early stopping\n",
    "    tolerance_level = 0\n",
    "    early_stopping_activated = 0\n",
    "    epoch = 0\n",
    "    MAX_MODEL = None\n",
    "    MAX_PERFORMANCE_WEIGHTS = None\n",
    "\n",
    "    train_loss_all = []\n",
    "    valid_loss_all = []\n",
    "    train_accuracy_all = []\n",
    "    valid_accuracy_all = []\n",
    "\n",
    "    print(len(train_loader))\n",
    "\n",
    "    while( epoch <= epochs and early_stopping_activated == 0 ):\n",
    "        train_loss = 0\n",
    "        valid_loss = 0\n",
    "        train_acc = 0\n",
    "        val_acc = 0\n",
    "        \n",
    "        # Training Part\n",
    "        # Always have this line to ensure proper training\n",
    "        net.train()\n",
    "        actual_labels_train = []\n",
    "        pred_labels_train = []\n",
    "        all_targets = []\n",
    "        all_preds = []\n",
    "\n",
    "        for i, (data, actual) in enumerate(train_loader):\n",
    "            # Reshape to single dimension\n",
    "            data = data.reshape(-1, 784)\n",
    "\n",
    "            # Push all variables to cuda\n",
    "            if(torch.cuda.is_available()):\n",
    "                data, actual =  data.float().to(device), actual.to(device)\n",
    "\n",
    "            output, _ = net(data.float())\n",
    "            loss =  loss_criteria(output, actual)\n",
    "            # Track loss\n",
    "            train_loss += loss.item()\n",
    "            train_predictions = func.softmax(output, dim = 1).argmax(dim=1, keepdim=True)            \n",
    "            actual_labels_train.append(actual.cpu().numpy())\n",
    "            pred_labels_train.append(train_predictions.cpu().numpy())\n",
    "\n",
    "            optimizer.zero_grad() # reset gradients\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        all_targets = np.concatenate(actual_labels_train, axis=0)\n",
    "        all_preds = np.concatenate(pred_labels_train, axis=0)\n",
    "        train_acc = accuracy_score(all_preds, all_targets)\n",
    "        train_accuracy_all.append(train_acc)\n",
    "\n",
    "\n",
    "        # Evaluation Part\n",
    "        # Always have this line to ensure proper evaluation \n",
    "        if(validation_process):\n",
    "          net.eval()\n",
    "          \n",
    "          actual_labels_all = []\n",
    "          pred_labels_all = []\n",
    "          all_targets = []\n",
    "          all_preds = []\n",
    "          # Now do validation and keep track of valid loss\n",
    "          with torch.no_grad():\n",
    "              for i, (data, labels) in enumerate(valid_loader):\n",
    "                  # Reshape to single dimension\n",
    "                  data = data.reshape(-1, 784)\n",
    "\n",
    "                  if(torch.cuda.is_available()):\n",
    "                      data, labels = data.float().to(device), labels.to(device)\n",
    "\n",
    "                  # FP     \n",
    "                  val_logits, _ = net.forward(data)\n",
    "                  v_loss = loss_criteria(val_logits, labels)\n",
    "                  valid_loss += v_loss.item()\n",
    "                  val_predictions = func.softmax(val_logits, dim = 1).argmax(dim=1, keepdim=True)\n",
    "                  actual_labels_all.append(labels.cpu().numpy())\n",
    "                  pred_labels_all.append(val_predictions.cpu().numpy())\n",
    "                  \n",
    "          all_targets = np.concatenate(actual_labels_all, axis=0)\n",
    "          all_preds = np.concatenate(pred_labels_all, axis=0)\n",
    "          val_acc = accuracy_score(all_preds, all_targets)\n",
    "          valid_accuracy_all.append(val_acc)\n",
    "          \n",
    "          if(epoch % 5 == 0):\n",
    "            print(f'Epoch {epoch} \\t Train loss(on avg per batch): {round(train_loss/len(train_loader), 6) } \\t Train Accuracy: {round(train_acc*100, 6)}% \\t Validation loss(on avg per batch): {round(valid_loss/len(valid_loader),6)} \\t Validation Accuracy: {round(val_acc*100,6)}%')\n",
    "\n",
    "          train_loss_all.append(train_loss/len(train_loader))\n",
    "          valid_loss_all.append(valid_loss/len(valid_loader)) \n",
    "          \n",
    "          if(val_acc*100 > max_performance):\n",
    "              max_performance = val_acc*100\n",
    "              tolerance_level = 0\n",
    "              # Save Model\n",
    "              MAX_PERFORMANCE_WEIGHTS = net.state_dict()\n",
    "              MAX_MODEL = net\n",
    "              \n",
    "          else:\n",
    "              tolerance_level+=1\n",
    "              if(tolerance_level >= 10):\n",
    "                  early_stopping_activated = 1\n",
    "                  print('Early Stopping activated - no improvement in validation accuracy for the past 10 epochs. Using model stage before 10 epochs for further use!')\n",
    "        epoch+=1\n",
    "\n",
    "    if(not validation_process):\n",
    "        # Save Model at end\n",
    "        MAX_PERFORMANCE_WEIGHTS = net.state_dict()\n",
    "        MAX_MODEL = net\n",
    "\n",
    "    print('Best Performance on Validation data achived till now :', max_performance)\n",
    "    \n",
    "    return MAX_MODEL, MAX_PERFORMANCE_WEIGHTS, train_accuracy_all, valid_accuracy_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jSuPv7g6_aQn",
    "outputId": "c5062269-ad1c-49e5-c295-25f78ee7ccaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline_q1(\n",
      "  (fp_input): Linear(in_features=784, out_features=1024, bias=True)\n",
      "  (fp1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (fp2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (fp3): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (fp4): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (fp_output): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (ReLU): ReLU()\n",
      ")\n",
      "469\n",
      "Epoch 0 \t Train loss(on avg per batch): 0.262295 \t Train Accuracy: 92.883333% \t Validation loss(on avg per batch): 0.132191 \t Validation Accuracy: 96.0%\n",
      "Best Performance on Validation data achived till now : 96.98\n"
     ]
    }
   ],
   "source": [
    "net = None\n",
    "net = baseline_q1()\n",
    "if(torch.cuda.is_available()):\n",
    "  net = net.float().to(device)\n",
    "else:\n",
    "  net = net.float()\n",
    "net.apply(he_weights)\n",
    "loss_func = nn.CrossEntropyLoss() # Here Sparse CLE is correct, but in pytorch nn.CrossEntropyLoss accepts ground truth labels directly as integers in [0, N_CLASSES] (no need to onehot encode the labels):\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = 0.001)\n",
    "print(net)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "q1_net_baseline, q1_weights_baseline, train_accuracy_all_baseline, valid_accuracy_all_baseline = run_network(net, epochs = 1, loss_criteria = loss_func, optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "omQTF_liAeY2",
    "outputId": "4bbeed95-fe90-489f-fdca-533db1e24418"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.save(q1_weights_baseline, '/content/drive/My Drive/Projects and research stuffs/DLS Assignments/Assignment 4/Q1 Baseline Weights')\n",
    "net = None\n",
    "net = baseline_q1()\n",
    "net.load_state_dict( torch.load('/content/drive/My Drive/Projects and research stuffs/DLS Assignments/Assignment 4/Q1 Baseline Weights', map_location=torch.device('cpu')) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGrNUdGVOO7V"
   },
   "source": [
    "#### 6 Other D values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "Hab_QZ0cYxre"
   },
   "outputs": [],
   "source": [
    "# net1 = copy.deepcopy(net)\n",
    "# print(sum(p.numel() for p in net.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "hNc06Lxj0FPD"
   },
   "outputs": [],
   "source": [
    "w1 = net.fp_input.weight\n",
    "w2 = net.fp1.weight\n",
    "w3 = net.fp2.weight\n",
    "w4 = net.fp3.weight\n",
    "w5 = net.fp4.weight\n",
    "\n",
    "bias1 = net.fp_input.bias\n",
    "bias2 = net.fp1.bias\n",
    "bias3 = net.fp2.bias\n",
    "bias4 = net.fp3.bias\n",
    "bias5 = net.fp4.bias\n",
    "\n",
    "\n",
    "U1, S1, V_t1 = torch.linalg.svd(w1)\n",
    "U1, S1, V_t1 = U1.detach().cpu().numpy(), S1.detach().cpu().numpy(), V_t1.detach().cpu().numpy()\n",
    "\n",
    "U2, S2, V_t2 = torch.linalg.svd(w2)\n",
    "U2, S2, V_t2 = U2.detach().cpu().numpy(), S2.detach().cpu().numpy(), V_t2.detach().cpu().numpy()\n",
    "\n",
    "U3, S3, V_t3 = torch.linalg.svd(w3)\n",
    "U3, S3, V_t3 = U3.detach().cpu().numpy(), S3.detach().cpu().numpy(), V_t3.detach().cpu().numpy()\n",
    "\n",
    "U4, S4, V_t4 = torch.linalg.svd(w4)\n",
    "U4, S4, V_t4 = U4.detach().cpu().numpy(), S4.detach().cpu().numpy(), V_t4.detach().cpu().numpy()\n",
    "\n",
    "U5, S5, V_t5 = torch.linalg.svd(w5)\n",
    "U5, S5, V_t5 = U5.detach().cpu().numpy(), S5.detach().cpu().numpy(), V_t5.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "o_FsE6rpKZDo"
   },
   "outputs": [],
   "source": [
    "# Predictions using Low-rank approximate of weight matrix of input to h5 hidden layers\n",
    "def convert_low_rank_matrix(low_rank_value, U, S, V_t):\n",
    "  U_hat = U[:, :low_rank_value]\n",
    "  S_hat = S[:low_rank_value]\n",
    "  S_hat = np.diag(S_hat)\n",
    "  V_t_hat = V_t[:low_rank_value, :]\n",
    "  w_hat = np.dot(U_hat, np.dot(S_hat, V_t_hat))\n",
    "  return w_hat\n",
    "\n",
    "def count_total_parameters_active(net):\n",
    "  total_nonzero = 0\n",
    "  for name, p in net.named_parameters():\n",
    "    tensor = p.data.cpu().numpy()\n",
    "    nz_count = np.count_nonzero(tensor)\n",
    "    total_nonzero += nz_count\n",
    "  return total_nonzero\n",
    "\n",
    "\n",
    "def predict_from_low_rank_approx(d_net, loss_criteria, low_rank_value, U1, S1, V_t1, U2, S2, V_t2, U3, S3, V_t3, U4, S4, V_t4, U5, S5, V_t5, validation_process=1):\n",
    "  w1_hat = convert_low_rank_matrix(low_rank_value, U1, S1, V_t1)\n",
    "  # print(w1_hat)\n",
    "  w2_hat = convert_low_rank_matrix(low_rank_value, U2, S2, V_t2)\n",
    "  w3_hat = convert_low_rank_matrix(low_rank_value, U3, S3, V_t3)\n",
    "  w4_hat = convert_low_rank_matrix(low_rank_value, U4, S4, V_t4)\n",
    "  w5_hat = convert_low_rank_matrix(low_rank_value, U5, S5, V_t5)\n",
    "  # Evaluation Part\n",
    "  # Always have this line to ensure proper evaluation \n",
    "  if(validation_process):\n",
    "    d_net.eval()\n",
    "\n",
    "    actual_labels_all = []\n",
    "    pred_labels_all = []\n",
    "    all_targets = []\n",
    "    all_preds = []\n",
    "    valid_accuracy_all = []\n",
    "    valid_loss_all = []\n",
    "    valid_loss = 0\n",
    "    # Now do validation and keep track of valid loss\n",
    "    with torch.no_grad():\n",
    "        for i, (data, labels) in enumerate(valid_loader):\n",
    "            # Reshape to single dimension\n",
    "            data = data.reshape(-1, 784)\n",
    "\n",
    "            if(torch.cuda.is_available()):\n",
    "                data, labels = data.float().to(device), labels.to(device)\n",
    "\n",
    "            # FP and change weights to lower rank matrix\n",
    "            d_net.fp_input.weight = nn.Parameter(torch.tensor(w1_hat))\n",
    "            d_net.fp1.weight = nn.Parameter(torch.tensor(w2_hat))\n",
    "            d_net.fp2.weight = nn.Parameter(torch.tensor(w3_hat))\n",
    "            d_net.fp3.weight = nn.Parameter(torch.tensor(w4_hat))\n",
    "            d_net.fp4.weight = nn.Parameter(torch.tensor(w5_hat)) \n",
    "            if(torch.cuda.is_available()):\n",
    "              d_net = d_net.to(device)         \n",
    "            val_logits, _ = d_net.forward(data)\n",
    "            if(torch.cuda.is_available()):\n",
    "              val_logits = val_logits.to(device)\n",
    "            v_loss = loss_criteria(val_logits, labels)\n",
    "            valid_loss += v_loss.item()\n",
    "            val_predictions = func.softmax(val_logits, dim = 1).argmax(dim=1, keepdim=True)\n",
    "            actual_labels_all.append(labels.cpu().numpy())\n",
    "            pred_labels_all.append(val_predictions.cpu().numpy())\n",
    "            \n",
    "    all_targets = np.concatenate(actual_labels_all, axis=0)\n",
    "    all_preds = np.concatenate(pred_labels_all, axis=0)\n",
    "    val_acc = accuracy_score(all_preds, all_targets)\n",
    "    valid_accuracy_all.append(val_acc)    \n",
    "    valid_loss_all.append(valid_loss/len(valid_loader)) \n",
    "  \n",
    "  total_params = count_total_parameters_active(d_net)\n",
    "  return valid_accuracy_all, total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "H73YwmfrN-k-"
   },
   "outputs": [],
   "source": [
    "# tU, tS, tV =  torch.linalg.svd(w1)\n",
    "# print(tU.shape, tS.shape, tV.shape)\n",
    "# rec = tU[:,:20]@(torch.diag(tS[:20])@tV[:20,:])\n",
    "# print( rec.shape, rec )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "0ZSHXDMe0Gmb"
   },
   "outputs": [],
   "source": [
    "test_acc_d10, total_params_d10  = predict_from_low_rank_approx(net, loss_func, 10, U1, S1, V_t1, U2, S2, V_t2, U3, S3, V_t3, U4, S4, V_t4, U5, S5, V_t5, validation_process=1)\n",
    "test_acc_d20, total_params_d20  = predict_from_low_rank_approx(net, loss_func, 20, U1, S1, V_t1, U2, S2, V_t2, U3, S3, V_t3, U4, S4, V_t4, U5, S5, V_t5, validation_process=1)\n",
    "test_acc_d50, total_params_d50  = predict_from_low_rank_approx(net, loss_func, 50, U1, S1, V_t1, U2, S2, V_t2, U3, S3, V_t3, U4, S4, V_t4, U5, S5, V_t5, validation_process=1)\n",
    "test_acc_d100, total_params_d100  = predict_from_low_rank_approx(net, loss_func, 100, U1, S1, V_t1, U2, S2, V_t2, U3, S3, V_t3, U4, S4, V_t4, U5, S5, V_t5, validation_process=1)\n",
    "test_acc_d200, total_params_d200  = predict_from_low_rank_approx(net, loss_func, 200, U1, S1, V_t1, U2, S2, V_t2, U3, S3, V_t3, U4, S4, V_t4, U5, S5, V_t5, validation_process=1)\n",
    "test_acc_d784, total_params_d784  = predict_from_low_rank_approx(net, loss_func, 784, U1, S1, V_t1, U2, S2, V_t2, U3, S3, V_t3, U4, S4, V_t4, U5, S5, V_t5, validation_process=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2s_-vVmnSoaO"
   },
   "source": [
    "#### Bar-chart analysis of test accuracies and number of parameters in different D values and baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "xTR0xRMt0Gos"
   },
   "outputs": [],
   "source": [
    "compare_df = pd.DataFrame(columns = ['D-value', 'Number of parameters', 'Test accuracy'])\n",
    "compare_df['D-value'] = [10, 20, 50, 100, 200, 784, 'Baseline']\n",
    "compare_df['Number of parameters'] = [total_params_d10, total_params_d20, total_params_d50, total_params_d100, total_params_d200, total_params_d784, sum(p.numel() for p in net.parameters())]\n",
    "compare_df['Test accuracy'] = [test_acc_d10[0]*100, test_acc_d20[0]*100, test_acc_d50[0]*100, test_acc_d100[0]*100, test_acc_d200[0]*100, test_acc_d784[0]*100, valid_accuracy_all_baseline[-1]*100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "65yFAYwy0GrN"
   },
   "outputs": [],
   "source": [
    "# sns.barplot(data = compare_df, x='D-value', y='Number of parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "wINYlaSO0Gtp",
    "outputId": "9874bb42-72b2-4cd9-8bec-6c15e48db989"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fb97c883890>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(data = compare_df, x='D-value', y='Test accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "vhdx0XSl0GwV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eF5cPqhe9jk"
   },
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SeyIsJdf0G0s",
    "outputId": "9e0556a2-2112-483d-a2d1-eb1d0c5012bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Should be W.t*X\n",
    "# Load previous weights\n",
    "net = None\n",
    "net = baseline_q1()\n",
    "net.load_state_dict( torch.load('/content/drive/My Drive/Projects and research stuffs/DLS Assignments/Assignment 4/Q1 Baseline Weights', map_location=torch.device('cpu')) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "NPDtuH6I0G2-"
   },
   "outputs": [],
   "source": [
    "class low_rank_nn_q2(nn.Module):\n",
    "    \n",
    "    def __init__(self, u1, v1, u2, v2, u3, v3, u4, v4, u5, v5, bias1, bias2, bias3, bias4, bias5):\n",
    "        super().__init__()\n",
    "        self.u1 = nn.Parameter(u1, requires_grad=True)\n",
    "        self.v1 = nn.Parameter(v1, requires_grad=True)\n",
    "        self.u2 = nn.Parameter(u2, requires_grad=True)\n",
    "        self.v2 = nn.Parameter(v2, requires_grad=True)\n",
    "        self.u3 = nn.Parameter(u3, requires_grad=True)\n",
    "        self.v3 = nn.Parameter(v3, requires_grad=True)\n",
    "        self.u4 = nn.Parameter(u4, requires_grad=True)\n",
    "        self.v4 = nn.Parameter(v4, requires_grad=True)\n",
    "        self.u5 = nn.Parameter(u5, requires_grad=True)\n",
    "        self.v5 = nn.Parameter(v5, requires_grad=True)\n",
    "\n",
    "        self.bias1 = nn.Parameter(bias1, requires_grad=True)\n",
    "        self.bias2 = nn.Parameter(bias2, requires_grad=True)\n",
    "        self.bias3 = nn.Parameter(bias3, requires_grad=True)\n",
    "        self.bias4 = nn.Parameter(bias4, requires_grad=True)\n",
    "        self.bias5 = nn.Parameter(bias5, requires_grad=True)\n",
    "\n",
    "        # self.u1 = nn.Linear(784, 20) #I, O\n",
    "        # self.v1 = nn.Linear(20, 1024) # I, O\n",
    "        \n",
    "        # self.u2 = nn.Linear(1024, 20)\n",
    "        # self.v2 = nn.Linear(20, 1024)\n",
    "        \n",
    "        # self.u3 = nn.Linear(1024, 20)\n",
    "        # self.v3 = nn.Linear(20, 1024)\n",
    "        \n",
    "        # self.u4 = nn.Linear(1024, 20)\n",
    "        # self.v4 = nn.Linear(20, 1024)\n",
    "        \n",
    "        # self.u5 = nn.Linear(1024, 20)\n",
    "        # self.v5 = nn.Linear(20, 1024)\n",
    "        \n",
    "        self.fp_output = nn.Linear(1024, 10)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.ReLU = nn.ReLU()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        to_save_layerop = {}\n",
    "        input = torch.matmul(input, torch.matmul(self.u1, self.v1).T)  + self.bias1 \n",
    "        input = self.ReLU(input)\n",
    "\n",
    "        input = torch.matmul(input, torch.matmul(self.u2, self.v2).T)  + self.bias2\n",
    "        input = self.ReLU(input)\n",
    "\n",
    "        input = torch.matmul(input, torch.matmul(self.u3, self.v3).T)  + self.bias3\n",
    "        input = self.ReLU(input)\n",
    "\n",
    "        input = torch.matmul(input, torch.matmul(self.u4, self.v4).T)  + self.bias4\n",
    "        input = self.ReLU(input)\n",
    "\n",
    "        input = torch.matmul(input, torch.matmul(self.u5, self.v5).T)  + self.bias5\n",
    "        input = self.ReLU(input)\n",
    "\n",
    "        input = self.fp_output(input)\n",
    "        to_save_layerop['fp_output_before_softmax'] = input\n",
    "\n",
    "        # MOST IMPORTANT REALIZATION...USE SOFTMAX AFTER CALCULATING CCE LOSS, THE CCE LOSS EXPECTS LOGITS AND NOT SOFTMAX OUTPUTS.\n",
    "        return input, to_save_layerop    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "qWZN3cdu0G5t"
   },
   "outputs": [],
   "source": [
    "# Use weights from d20 of Q1\n",
    "def get_lower_uv(U, S, V):\n",
    "  low_rank_value = 20\n",
    "  U_hat = U[:, :low_rank_value]\n",
    "  S_hat = S[:low_rank_value]\n",
    "  S_hat = np.diag(S_hat)\n",
    "  V_t_hat = np.dot(S_hat, V[:low_rank_value, :])\n",
    "  return U_hat, V_t_hat\n",
    "\n",
    "\n",
    "w1 = net.fp_input.weight\n",
    "w2 = net.fp1.weight\n",
    "w3 = net.fp2.weight\n",
    "w4 = net.fp3.weight\n",
    "w5 = net.fp4.weight\n",
    "U1, S1, V_t1 = torch.linalg.svd(w1)\n",
    "U1, S1, V_t1 = U1.detach().cpu().numpy(), S1.detach().cpu().numpy(), V_t1.detach().cpu().numpy()\n",
    "\n",
    "U2, S2, V_t2 = torch.linalg.svd(w2)\n",
    "U2, S2, V_t2 = U2.detach().cpu().numpy(), S2.detach().cpu().numpy(), V_t2.detach().cpu().numpy()\n",
    "\n",
    "U3, S3, V_t3 = torch.linalg.svd(w3)\n",
    "U3, S3, V_t3 = U3.detach().cpu().numpy(), S3.detach().cpu().numpy(), V_t3.detach().cpu().numpy()\n",
    "\n",
    "U4, S4, V_t4 = torch.linalg.svd(w4)\n",
    "U4, S4, V_t4 = U4.detach().cpu().numpy(), S4.detach().cpu().numpy(), V_t4.detach().cpu().numpy()\n",
    "\n",
    "U5, S5, V_t5 = torch.linalg.svd(w5)\n",
    "U5, S5, V_t5 = U5.detach().cpu().numpy(), S5.detach().cpu().numpy(), V_t5.detach().cpu().numpy()\n",
    "\n",
    "U1, V_t1 = get_lower_uv(U1, S1, V_t1)\n",
    "U2, V_t2 = get_lower_uv(U2, S2, V_t2)\n",
    "U3, V_t3 = get_lower_uv(U3, S3, V_t3)\n",
    "U4, V_t4 = get_lower_uv(U4, S4, V_t4)\n",
    "U5, V_t5 = get_lower_uv(U5, S5, V_t5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "N_vNH4QgXRg5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "bQynd4B-0A0x"
   },
   "outputs": [],
   "source": [
    "def run_network(net, epochs, loss_criteria, optimizer, validation_process=True):\n",
    "    # A minor difference is that the implementation of CrossEntrypyLoss implicitly applies a softmax activation followed by a log transformation \n",
    "    # but NLLLoss does not.\n",
    "\n",
    "    # To keep best performance value\n",
    "    max_performance = float('-inf')\n",
    "    \n",
    "    # For early stopping\n",
    "    tolerance_level = 0\n",
    "    early_stopping_activated = 0\n",
    "    epoch = 0\n",
    "    MAX_MODEL = None\n",
    "    MAX_PERFORMANCE_WEIGHTS = None\n",
    "\n",
    "    train_loss_all = []\n",
    "    valid_loss_all = []\n",
    "    train_accuracy_all = []\n",
    "    valid_accuracy_all = []\n",
    "\n",
    "    print(len(train_loader))\n",
    "\n",
    "    while( epoch <= epochs and early_stopping_activated == 0 ):\n",
    "        train_loss = 0\n",
    "        valid_loss = 0\n",
    "        train_acc = 0\n",
    "        val_acc = 0\n",
    "        \n",
    "        # Training Part\n",
    "        # Always have this line to ensure proper training\n",
    "        net.train()\n",
    "        actual_labels_train = []\n",
    "        pred_labels_train = []\n",
    "        all_targets = []\n",
    "        all_preds = []\n",
    "\n",
    "        for i, (data, actual) in enumerate(train_loader):\n",
    "            # Reshape to single dimension\n",
    "            data = data.reshape(-1, 784)\n",
    "\n",
    "            # Push all variables to cuda\n",
    "            if(torch.cuda.is_available()):\n",
    "                data, actual =  data.float().to(device), actual.to(device)\n",
    "\n",
    "            output, _ = net(data.float())\n",
    "            loss =  loss_criteria(output, actual)\n",
    "            # Track loss\n",
    "            train_loss += loss.item()\n",
    "            train_predictions = func.softmax(output, dim = 1).argmax(dim=1, keepdim=True)            \n",
    "            actual_labels_train.append(actual.cpu().numpy())\n",
    "            pred_labels_train.append(train_predictions.cpu().numpy())\n",
    "\n",
    "            optimizer.zero_grad() # reset gradients\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        all_targets = np.concatenate(actual_labels_train, axis=0)\n",
    "        all_preds = np.concatenate(pred_labels_train, axis=0)\n",
    "        train_acc = accuracy_score(all_preds, all_targets)\n",
    "        train_accuracy_all.append(train_acc)\n",
    "\n",
    "\n",
    "        # Evaluation Part\n",
    "        # Always have this line to ensure proper evaluation \n",
    "        if(validation_process):\n",
    "          net.eval()\n",
    "          \n",
    "          actual_labels_all = []\n",
    "          pred_labels_all = []\n",
    "          all_targets = []\n",
    "          all_preds = []\n",
    "          # Now do validation and keep track of valid loss\n",
    "          with torch.no_grad():\n",
    "              for i, (data, labels) in enumerate(valid_loader):\n",
    "                  # Reshape to single dimension\n",
    "                  data = data.reshape(-1, 784)\n",
    "\n",
    "                  if(torch.cuda.is_available()):\n",
    "                      data, labels = data.float().to(device), labels.to(device)\n",
    "\n",
    "                  # FP     \n",
    "                  val_logits, _ = net.forward(data)\n",
    "                  v_loss = loss_criteria(val_logits, labels)\n",
    "                  valid_loss += v_loss.item()\n",
    "                  val_predictions = func.softmax(val_logits, dim = 1).argmax(dim=1, keepdim=True)\n",
    "                  actual_labels_all.append(labels.cpu().numpy())\n",
    "                  pred_labels_all.append(val_predictions.cpu().numpy())\n",
    "                  \n",
    "          all_targets = np.concatenate(actual_labels_all, axis=0)\n",
    "          all_preds = np.concatenate(pred_labels_all, axis=0)\n",
    "          val_acc = accuracy_score(all_preds, all_targets)\n",
    "          valid_accuracy_all.append(val_acc)\n",
    "          \n",
    "          if(epoch % 5 == 0):\n",
    "            print(f'Epoch {epoch} \\t Train loss(on avg per batch): {round(train_loss/len(train_loader), 6) } \\t Train Accuracy: {round(train_acc*100, 6)}% \\t Validation loss(on avg per batch): {round(valid_loss/len(valid_loader),6)} \\t Validation Accuracy: {round(val_acc*100,6)}%')\n",
    "\n",
    "          train_loss_all.append(train_loss/len(train_loader))\n",
    "          valid_loss_all.append(valid_loss/len(valid_loader)) \n",
    "          \n",
    "          if(val_acc*100 > max_performance):\n",
    "              max_performance = val_acc*100\n",
    "              tolerance_level = 0\n",
    "              # Save Model\n",
    "              MAX_PERFORMANCE_WEIGHTS = net.state_dict()\n",
    "              MAX_MODEL = net\n",
    "              \n",
    "          else:\n",
    "              tolerance_level+=1\n",
    "              if(tolerance_level >= 10):\n",
    "                  early_stopping_activated = 1\n",
    "                  print('Early Stopping activated - no improvement in validation accuracy for the past 10 epochs. Using model stage before 10 epochs for further use!')\n",
    "        epoch+=1\n",
    "\n",
    "    if(not validation_process):\n",
    "        # Save Model at end\n",
    "        MAX_PERFORMANCE_WEIGHTS = net.state_dict()\n",
    "        MAX_MODEL = net\n",
    "\n",
    "    print('Best Performance on Validation data achived till now :', max_performance)\n",
    "    \n",
    "    return MAX_MODEL, MAX_PERFORMANCE_WEIGHTS, train_accuracy_all, valid_accuracy_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5z89qoqB0G8Z",
    "outputId": "34ddfeb5-fca9-40bc-bfad-857f778e6c5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low_rank_nn_q2(\n",
      "  (fp_output): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (ReLU): ReLU()\n",
      ")\n",
      "469\n",
      "Epoch 0 \t Train loss(on avg per batch): 0.246048 \t Train Accuracy: 93.688333% \t Validation loss(on avg per batch): 0.116169 \t Validation Accuracy: 97.4%\n",
      "Epoch 5 \t Train loss(on avg per batch): 0.030705 \t Train Accuracy: 99.036667% \t Validation loss(on avg per batch): 0.095366 \t Validation Accuracy: 97.96%\n",
      "Epoch 10 \t Train loss(on avg per batch): 0.01621 \t Train Accuracy: 99.498333% \t Validation loss(on avg per batch): 0.133767 \t Validation Accuracy: 97.71%\n",
      "Epoch 15 \t Train loss(on avg per batch): 0.011358 \t Train Accuracy: 99.658333% \t Validation loss(on avg per batch): 0.127824 \t Validation Accuracy: 98.23%\n",
      "Epoch 20 \t Train loss(on avg per batch): 0.006012 \t Train Accuracy: 99.796667% \t Validation loss(on avg per batch): 0.18338 \t Validation Accuracy: 98.04%\n",
      "Epoch 25 \t Train loss(on avg per batch): 0.007256 \t Train Accuracy: 99.751667% \t Validation loss(on avg per batch): 0.162894 \t Validation Accuracy: 98.14%\n",
      "Early Stopping activated - no improvement in validation accuracy for the past 10 epochs. Using model stage before 10 epochs for further use!\n",
      "Best Performance on Validation data achived till now : 98.22999999999999\n"
     ]
    }
   ],
   "source": [
    "# Set weights\n",
    "# U1, V_t1\n",
    "u1 = torch.tensor(U1) \n",
    "v1 = torch.tensor(V_t1) \n",
    "u2 = torch.tensor(U2) \n",
    "v2 = torch.tensor(V_t2) \n",
    "u3 = torch.tensor(U3) \n",
    "v3 = torch.tensor(V_t3) \n",
    "u4 = torch.tensor(U4) \n",
    "v4 = torch.tensor(V_t4) \n",
    "u5 = torch.tensor(U5) \n",
    "v5 = torch.tensor(V_t5)\n",
    "\n",
    "bias_input = net.fp_input.bias\n",
    "bias2 = net.fp1.bias\n",
    "bias3 = net.fp2.bias\n",
    "bias4 = net.fp3.bias\n",
    "bias5 = net.fp4.bias\n",
    "\n",
    "if(torch.cuda.is_available()):\n",
    "  u1, v1, u2, v2, u3, v3, u4, v4, u5, v5 =  u1.to(device), v1.to(device), u2.to(device), v2.to(device), u3.to(device), v3.to(device), u4.to(device), v4.to(device), u5.to(device), v5.to(device)\n",
    "  bias_input, bias2, bias3, bias4, bias5 =  bias_input.to(device), bias2.to(device), bias3.to(device), bias4.to(device), bias5.to(device)\n",
    "\n",
    "net_q2 = None\n",
    "net_q2 = low_rank_nn_q2(u1, v1, u2, v2, u3, v3, u4, v4, u5, v5, bias_input, bias2, bias3, bias4, bias5)\n",
    "if(torch.cuda.is_available()):\n",
    "  net_q2 = net_q2.float().to(device)\n",
    "else:\n",
    "  net_q2 = net_q2.float()\n",
    "loss_func = nn.CrossEntropyLoss() # Here Sparse CLE is correct, but in pytorch nn.CrossEntropyLoss accepts ground truth labels directly as integers in [0, N_CLASSES] (no need to onehot encode the labels):\n",
    "optimizer = torch.optim.Adam(net_q2.parameters(), lr = 0.0001)\n",
    "print(net_q2)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "q2_net_baseline, q2_weights_baseline, train_accuracy_all_q2, valid_accuracy_all_q2 = run_network(net_q2, epochs = 30, loss_criteria = loss_func, optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "Rsvny_5oevjt",
    "outputId": "395d3350-7e45-466a-f1bf-73a7f022f64b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs_range = [i for i in range(26)] # last epoch was at 26\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.plot(epochs_range, valid_accuracy_all_q2, color=\"blue\", linewidth=2)\n",
    "plt.title('Test-time Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "1irnhCGt0G-w"
   },
   "outputs": [],
   "source": [
    "# torch.save(q2_weights_baseline, '/content/drive/My Drive/Projects and research stuffs/DLS Assignments/Assignment 4/Q2 Finetuned Weights')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "AaNcWzgu0HBY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuW6EV85zGEK"
   },
   "source": [
    "## Problem 3: SVD on every Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xjcx6964zciu",
    "outputId": "503125cf-7ad9-4ca4-b774-1e6be3f5e958"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load previous weights\n",
    "q1_net = None\n",
    "q1_net = baseline_q1()\n",
    "q1_net.load_state_dict( torch.load('/content/drive/My Drive/Projects and research stuffs/DLS Assignments/Assignment 4/Q1 Baseline Weights', map_location=torch.device('cpu')) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "rTvrl0yr0HD9"
   },
   "outputs": [],
   "source": [
    "def get_low_rank_weights(w1, w2, w3, w4, w5):\n",
    "  \n",
    "  U1, S1, V_t1 = torch.linalg.svd(w1)\n",
    "  U1, S1, V_t1 = U1.detach().cpu().numpy(), S1.detach().cpu().numpy(), V_t1.detach().cpu().numpy()\n",
    "\n",
    "  U2, S2, V_t2 = torch.linalg.svd(w2)\n",
    "  U2, S2, V_t2 = U2.detach().cpu().numpy(), S2.detach().cpu().numpy(), V_t2.detach().cpu().numpy()\n",
    "\n",
    "  U3, S3, V_t3 = torch.linalg.svd(w3)\n",
    "  U3, S3, V_t3 = U3.detach().cpu().numpy(), S3.detach().cpu().numpy(), V_t3.detach().cpu().numpy()\n",
    "\n",
    "  U4, S4, V_t4 = torch.linalg.svd(w4)\n",
    "  U4, S4, V_t4 = U4.detach().cpu().numpy(), S4.detach().cpu().numpy(), V_t4.detach().cpu().numpy()\n",
    "\n",
    "  U5, S5, V_t5 = torch.linalg.svd(w5)\n",
    "  U5, S5, V_t5 = U5.detach().cpu().numpy(), S5.detach().cpu().numpy(), V_t5.detach().cpu().numpy()\n",
    "\n",
    "  w1_hat = torch.tensor(convert_low_rank_matrix(20, U1, S1, V_t1))\n",
    "  w2_hat = torch.tensor(convert_low_rank_matrix(20, U2, S2, V_t2))\n",
    "  w3_hat = torch.tensor(convert_low_rank_matrix(20, U3, S3, V_t3))\n",
    "  w4_hat = torch.tensor(convert_low_rank_matrix(20, U4, S4, V_t4))\n",
    "  w5_hat = torch.tensor(convert_low_rank_matrix(20, U5, S5, V_t5))\n",
    "\n",
    "  return w1_hat, w2_hat, w3_hat, w4_hat, w5_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "8BjMWbeG0HG_"
   },
   "outputs": [],
   "source": [
    "class low_rank_nn_q3(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fp_output = nn.Linear(1024, 10)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        input = self.fp_output(input)\n",
    "\n",
    "        # MOST IMPORTANT REALIZATION...USE SOFTMAX AFTER CALCULATING CCE LOSS, THE CCE LOSS EXPECTS LOGITS AND NOT SOFTMAX OUTPUTS.\n",
    "        return input    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "ZtUYts7oMjuA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "_hLTj8nMMiZf"
   },
   "outputs": [],
   "source": [
    "class low_rank_nn_q3(nn.Module):\n",
    "    \n",
    "    def __init__(self, w_input_hat, bias_input, w2_hat, bias2, w3_hat, bias3, w4_hat, bias4, w5_hat, bias5):\n",
    "      super().__init__()\n",
    "      self.w_input_hat = nn.Parameter(w_input_hat, requires_grad=True)\n",
    "      self.w2_hat = nn.Parameter(w2_hat, requires_grad=True)\n",
    "      self.w3_hat = nn.Parameter(w3_hat, requires_grad=True)\n",
    "      self.w4_hat = nn.Parameter(w4_hat, requires_grad=True)\n",
    "      self.w5_hat = nn.Parameter(w5_hat, requires_grad=True)\n",
    "\n",
    "      self.bias_input = nn.Parameter(bias_input, requires_grad=True)\n",
    "      self.bias2 = nn.Parameter(bias2, requires_grad=True)\n",
    "      self.bias3 = nn.Parameter(bias3, requires_grad=True)\n",
    "      self.bias4 = nn.Parameter(bias4, requires_grad=True)\n",
    "      self.bias5 = nn.Parameter(bias5, requires_grad=True)\n",
    "\n",
    "      self.ReLU = nn.ReLU()\n",
    "      self.fp_output = nn.Linear(1024, 10)\n",
    "      \n",
    "    def forward(self, x1 ): # x1, w_input_hat, bias_input, w2_hat, bias2, w3_hat, bias3, w4_hat, bias4, w5_hat, bias5\n",
    "      z1 = torch.matmul( x1, self.w_input_hat.T) + self.bias_input\n",
    "      x2 = self.ReLU(z1)\n",
    "\n",
    "      z2 = torch.matmul(x2, self.w2_hat.T) + self.bias2\n",
    "      x3 = self.ReLU(z2)\n",
    "\n",
    "      z3 = torch.matmul(x3, self.w3_hat.T) + self.bias3\n",
    "      x4 = self.ReLU(z3)\n",
    "\n",
    "      z4 = torch.matmul(x4, self.w4_hat.T) + self.bias4\n",
    "      x5 = self.ReLU(z4)\n",
    "      \n",
    "      z5 = torch.matmul(x5, self.w5_hat) + self.bias5\n",
    "      x_output = self.ReLU(z5)\n",
    "    \n",
    "      y = self.fp_output(x_output)\n",
    "\n",
    "      all_x = [x1, x2, x3, x4, x5, x_output] # list of 6\n",
    "\n",
    "      # MOST IMPORTANT REALIZATION...USE SOFTMAX AFTER CALCULATING CCE LOSS, THE CCE LOSS EXPECTS LOGITS AND NOT SOFTMAX OUTPUTS.\n",
    "      return y, all_x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "h8XRWvFm3hj0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "Q9XCh8Ps3iT9"
   },
   "outputs": [],
   "source": [
    "# def get_relu_derivative(z):\n",
    "#   z[z<=0] = 0\n",
    "#   z[z>0] = 1\n",
    "#   return z\n",
    "\n",
    "# def custom_feedforward(x1, w_input_hat, bias_input, w2_hat, bias2, w3_hat, bias3, w4_hat, bias4, w5_hat, bias5):\n",
    "#   ReLU = nn.ReLU()\n",
    "#   z1 = torch.matmul( x1, w_input_hat.T) + bias_input\n",
    "#   x2 = ReLU(z1)\n",
    "\n",
    "#   z2 = torch.matmul(x2, w2_hat.T) + bias2\n",
    "#   x3 = ReLU(z2)\n",
    "\n",
    "#   z3 = torch.matmul(x3, w3_hat.T) + bias3\n",
    "#   x4 = ReLU(z3)\n",
    "\n",
    "#   z4 = torch.matmul(x4, w4_hat.T) + bias4\n",
    "#   x5 = ReLU(z4)\n",
    "  \n",
    "#   z5 = torch.matmul(x5, w5_hat) + bias5\n",
    "#   x_output = ReLU(z5) # x_output is input to last layer (softmax layer)\n",
    "\n",
    "#   # Append all x and weight values to list of list\n",
    "#   all_x = [x1, x2, x3, x4, x5, x_output] # list of 6\n",
    "#   all_z = [z1, z2, z3, z4, z5] # list of 6\n",
    "#   all_w = [w_input_hat, w2_hat, w3_hat, w4_hat, w5_hat] # list of 5\n",
    "#   all_bias = [bias_input, bias2, bias3, bias4, bias5] # list of 5\n",
    "\n",
    "#   return all_x, all_z, all_w, all_bias\n",
    "\n",
    "# def custom_backpropagation(x_layers, z_layers, w_layers, bias_layers, gradient_backward, lr): # Need list of x_layer of each layer, and their weights - from input layer to (excluding) last layer, till hidden layer 5\n",
    "#   for i in range(4, -1, -1): \n",
    "#     # iterate from last to first (6th or softmax layer is already done by torch function)\n",
    "#     # For weights\n",
    "#     # torch.Size([1024, 1024]) torch.Size([10, 1024]) torch.Size([10, 1024]) torch.Size([1024, 128]) torch.Size([1024, 128])\n",
    "#     print( w_layers[i].shape, w_layers[i+1].shape,  gradient_backward.shape, x_layers[i].shape, z_layers[i].shape)\n",
    "\n",
    "#     gradient_current =  torch.matmul(w_layers[i+1].T, gradient_backward) * get_relu_derivative(z_layers[i])\n",
    "#     w_layers[i] = w_layers[i] - lr*(torch.matmul(gradient_current, x_layers[i].T))*torch.eye(w_layers[i].shape[0], w_layers[i].shape[1])\n",
    "    \n",
    "#     # For bias\n",
    "#     # bias[j] -= gamma_bias * 1 * delta[j]\n",
    "#     bias_layers[i] = bias_layers[i] - lr*gradient_current*1 # input is just 1\n",
    "\n",
    "#     gradient_backward = gradient_current\n",
    "\n",
    "#   return w_layers, bias_layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "flcJCiOyQVtV"
   },
   "outputs": [],
   "source": [
    "def run_custom_network(net, w_input_hat, bias_input, w2_hat, bias2, w3_hat, bias3, w4_hat, bias4, w5_hat, bias5, epochs, loss_criteria, optimizer, learning_rate, validation_process=True):\n",
    "  # A minor difference is that the implementation of CrossEntrypyLoss implicitly applies a softmax activation followed by a log transformation \n",
    "  # but NLLLoss does not.\n",
    "  # To keep best performance value\n",
    "  max_performance = float('-inf')\n",
    "  \n",
    "  # For early stopping\n",
    "  tolerance_level = 0\n",
    "  early_stopping_activated = 0\n",
    "  epoch = 0\n",
    "  MAX_MODEL = None\n",
    "  MAX_PERFORMANCE_WEIGHTS = None\n",
    "\n",
    "  train_loss_all = []\n",
    "  valid_loss_all = []\n",
    "  train_accuracy_all = []\n",
    "  valid_accuracy_all = []\n",
    "\n",
    "  print(len(train_loader))\n",
    "\n",
    "  # # Set all initial parameters to random values\n",
    "  # bias_input = torch.rand(1)\n",
    "  # bias2 = torch.rand(1)\n",
    "  # bias3 = torch.rand(1)\n",
    "  # bias4 = torch.rand(1)\n",
    "  # bias5 = torch.rand(1)\n",
    "  # w1 = q1_net.fp_input.weight\n",
    "  # w2 = q1_net.fp1.weight\n",
    "  # w3 = q1_net.fp2.weight\n",
    "  # w4 = q1_net.fp3.weight\n",
    "  # w5 = q1_net.fp4.weight\n",
    "  # w_input_hat, w2_hat, w3_hat, w4_hat, w5_hat = get_low_rank_weights(w1, w2, w3, w4, w5) # get the weights from baseline model\n",
    "   \n",
    "\n",
    "  while( epoch <= epochs and early_stopping_activated == 0 ):\n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "    train_acc = 0\n",
    "    val_acc = 0\n",
    "    \n",
    "    # Training Part\n",
    "    # Always have this line to ensure proper training\n",
    "    net.train()\n",
    "    actual_labels_train = []\n",
    "    pred_labels_train = []\n",
    "    all_targets = []\n",
    "    all_preds = []\n",
    "\n",
    "    for i, (data, actual) in enumerate(train_loader):\n",
    "        # Reshape to single dimension\n",
    "        data = data.reshape(-1, 784).float()\n",
    "\n",
    "        # Push all variables to cuda\n",
    "        if(torch.cuda.is_available()):\n",
    "            data, actual =  data.to(device), actual.to(device)\n",
    "\n",
    "        output, all_x = net.forward(data) # , w_input_hat, bias_input, w2_hat, bias2, w3_hat, bias3, w4_hat, bias4, w5_hat, bias5\n",
    "\n",
    "        loss =  loss_criteria(output, actual)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights manually\n",
    "        with torch.no_grad():\n",
    "          w5_hat = net.w5_hat - learning_rate*net.w5_hat.grad* 1\n",
    "          w4_hat = net.w4_hat - learning_rate*net.w4_hat.grad* 1\n",
    "          w3_hat = net.w3_hat - learning_rate*net.w3_hat.grad* 1\n",
    "          w2_hat = net.w2_hat - learning_rate*net.w2_hat.grad* 1\n",
    "          w_input_hat = net.w_input_hat - learning_rate*net.w_input_hat.grad*1\n",
    "          \n",
    "          # net.w5_hat = net.w5_hat - learning_rate*torch.matmul(net.w5_hat.grad, all_x[4].T)*torch.ones(net.w5_hat.shape[0], net.w5_hat.shape[1])\n",
    "          # net.w4_hat = net.w4_hat - learning_rate*torch.matmul(net.w4_hat.grad, all_x[3].T)*torch.ones(net.w4_hat.shape[0], net.w4_hat.shape[1])\n",
    "          # net.w3_hat = net.w3_hat - learning_rate*torch.matmul(net.w3_hat.grad, all_x[2].T)*torch.ones(net.w3_hat.shape[0], net.w3_hat.shape[1])\n",
    "          # net.w2_hat = net.w2_hat - learning_rate*torch.matmul(net.w2_hat.grad, all_x[1].T)*torch.ones(net.w2_hat.shape[0], net.w2_hat.shape[1])\n",
    "          # net.w_input_hat = net.w_input_hat - learning_rate*torch.matmul(net.w_input_hat.grad, all_x[0].T)*torch.ones(net.w_input_hat.shape[0], net.w_input_hat.shape[1])\n",
    "          \n",
    "          # For bias update\n",
    "          bias_input = net.bias_input - learning_rate*net.bias_input.grad*1 # input is just 1\n",
    "          bias2 = net.bias2 - learning_rate*net.bias2.grad*1 # input is just 1\n",
    "          bias3 = net.bias3 - learning_rate*net.bias3.grad*1 # input is just 1\n",
    "          bias4 = net.bias4 - learning_rate*net.bias4.grad*1 # input is just 1\n",
    "          bias5 = net.bias5 - learning_rate*net.bias5.grad*1 # input is just 1\n",
    "        \n",
    "        # Perform SVD compression with updated weights\n",
    "        w_input_hat, w2_hat, w3_hat, w4_hat, w5_hat = get_low_rank_weights(w_input_hat, w2_hat, w3_hat, w4_hat, w5_hat)\n",
    "        \n",
    "        if(torch.cuda.is_available()):\n",
    "            w_input_hat, w2_hat, w3_hat, w4_hat, w5_hat =  w_input_hat.to(device), w2_hat.to(device), w3_hat.to(device), w4_hat.to(device), w5_hat.to(device)\n",
    "            bias_input, bias2, bias3, bias4, bias5 =  bias_input.to(device), bias2.to(device), bias3.to(device), bias4.to(device), bias5.to(device)\n",
    "\n",
    "        # Now set and change weights of network to these low-rank approximated weights\n",
    "        with torch.no_grad():\n",
    "          net.w_input_hat = nn.Parameter(w_input_hat, requires_grad=True)\n",
    "          net.w2_hat = nn.Parameter(w2_hat, requires_grad=True)\n",
    "          net.w3_hat = nn.Parameter(w3_hat, requires_grad=True)\n",
    "          net.w4_hat = nn.Parameter(w4_hat, requires_grad=True)\n",
    "          net.w5_hat = nn.Parameter(w5_hat, requires_grad=True)\n",
    "\n",
    "          net.bias_input = nn.Parameter(bias_input, requires_grad=True)\n",
    "          net.bias2 = nn.Parameter(bias2, requires_grad=True)\n",
    "          net.bias3 = nn.Parameter(bias3, requires_grad=True)\n",
    "          net.bias4 = nn.Parameter(bias4, requires_grad=True)\n",
    "          net.bias5 = nn.Parameter(bias5, requires_grad=True)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print('after update',w_input_hat, 'and', net.w_input_hat, 'also', any(any(row) for row in net.w_input_hat != temp))\n",
    "          \n",
    "        # Make Predictions at every epoch\n",
    "        train_predictions = func.softmax(output, dim = 1).argmax(dim=1, keepdim=True)\n",
    "        actual_labels_train.append(actual.cpu().numpy())\n",
    "        pred_labels_train.append(train_predictions.cpu().numpy())\n",
    "        \n",
    "    all_targets = np.concatenate(actual_labels_train, axis=0)\n",
    "    all_preds = np.concatenate(pred_labels_train, axis=0)\n",
    "    train_acc = accuracy_score(all_preds, all_targets)\n",
    "    train_accuracy_all.append(train_acc)\n",
    "\n",
    "    # Evaluation Part\n",
    "    # Always have this line to ensure proper evaluation \n",
    "    if(validation_process):\n",
    "      net.eval()\n",
    "      \n",
    "      actual_labels_all = []\n",
    "      pred_labels_all = []\n",
    "      all_targets = []\n",
    "      all_preds = []\n",
    "      # Now do validation and keep track of valid loss\n",
    "      with torch.no_grad():\n",
    "          for i, (data, labels) in enumerate(valid_loader):\n",
    "              # Reshape to single dimension\n",
    "              data = data.reshape(-1, 784)\n",
    "\n",
    "              if(torch.cuda.is_available()):\n",
    "                  data, labels = data.float().to(device), labels.to(device)\n",
    "\n",
    "              # FP     \n",
    "              val_logits, _ = net.forward(data)\n",
    "              v_loss = loss_criteria(val_logits, labels)\n",
    "              valid_loss += v_loss.item()\n",
    "              val_predictions = func.softmax(val_logits, dim = 1).argmax(dim=1, keepdim=True)\n",
    "              actual_labels_all.append(labels.cpu().numpy())\n",
    "              pred_labels_all.append(val_predictions.cpu().numpy())\n",
    "              \n",
    "      all_targets = np.concatenate(actual_labels_all, axis=0)\n",
    "      all_preds = np.concatenate(pred_labels_all, axis=0)\n",
    "      val_acc = accuracy_score(all_preds, all_targets)\n",
    "      valid_accuracy_all.append(val_acc)\n",
    "      \n",
    "    if(epoch % 1 == 0):\n",
    "      print(f'Epoch {epoch} \\t Train loss(on avg per batch): {round(train_loss/len(train_loader), 6) } \\t Train Accuracy: {round(train_acc*100, 6)}% \\t Validation loss(on avg per batch): {round(valid_loss/len(valid_loader),6)} \\t Validation Accuracy: {round(val_acc*100,6)}%')\n",
    "\n",
    "    train_loss_all.append(train_loss/len(train_loader))\n",
    "    valid_loss_all.append(valid_loss/len(valid_loader)) \n",
    "    \n",
    "    if(val_acc*100 > max_performance):\n",
    "        max_performance = val_acc*100\n",
    "        tolerance_level = 0\n",
    "        # Save Model\n",
    "        MAX_PERFORMANCE_WEIGHTS = net.state_dict()\n",
    "        MAX_MODEL = net\n",
    "        \n",
    "    else:\n",
    "        tolerance_level+=1\n",
    "        if(tolerance_level >= 10):\n",
    "            early_stopping_activated = 1\n",
    "            print('Early Stopping activated - no improvement in validation accuracy for the past 10 epochs. Using model stage before 10 epochs for further use!')\n",
    "    \n",
    "    epoch+=1\n",
    "\n",
    "  if(not validation_process):\n",
    "      # Save Model at end\n",
    "      MAX_PERFORMANCE_WEIGHTS = net.state_dict()\n",
    "      MAX_MODEL = net\n",
    "\n",
    "  print('Best Performance on Validation data achived till now :', max_performance)\n",
    "  \n",
    "  return MAX_MODEL, MAX_PERFORMANCE_WEIGHTS, train_accuracy_all, valid_accuracy_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5BYbk0ezzBuk",
    "outputId": "7198ce69-eae7-488c-ba71-a578416b0064"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low_rank_nn_q3(\n",
      "  (ReLU): ReLU()\n",
      "  (fp_output): Linear(in_features=1024, out_features=10, bias=True)\n",
      ")\n",
      "469\n",
      "Epoch 0 \t Train loss(on avg per batch): 1.499019 \t Train Accuracy: 55.895% \t Validation loss(on avg per batch): 1.128224 \t Validation Accuracy: 65.59%\n",
      "Epoch 1 \t Train loss(on avg per batch): 1.03626 \t Train Accuracy: 73.5% \t Validation loss(on avg per batch): 0.959198 \t Validation Accuracy: 81.67%\n",
      "Epoch 2 \t Train loss(on avg per batch): 0.905625 \t Train Accuracy: 83.188333% \t Validation loss(on avg per batch): 0.855965 \t Validation Accuracy: 83.28%\n",
      "Epoch 3 \t Train loss(on avg per batch): 0.808072 \t Train Accuracy: 84.576667% \t Validation loss(on avg per batch): 0.764874 \t Validation Accuracy: 84.94%\n",
      "Epoch 4 \t Train loss(on avg per batch): 0.718181 \t Train Accuracy: 86.031667% \t Validation loss(on avg per batch): 0.680066 \t Validation Accuracy: 86.43%\n",
      "Epoch 5 \t Train loss(on avg per batch): 0.634921 \t Train Accuracy: 87.931667% \t Validation loss(on avg per batch): 0.602201 \t Validation Accuracy: 88.73%\n",
      "Epoch 6 \t Train loss(on avg per batch): 0.556268 \t Train Accuracy: 90.413333% \t Validation loss(on avg per batch): 0.527302 \t Validation Accuracy: 91.2%\n",
      "Epoch 7 \t Train loss(on avg per batch): 0.481234 \t Train Accuracy: 92.461667% \t Validation loss(on avg per batch): 0.457337 \t Validation Accuracy: 92.88%\n",
      "Epoch 8 \t Train loss(on avg per batch): 0.411909 \t Train Accuracy: 93.863333% \t Validation loss(on avg per batch): 0.391785 \t Validation Accuracy: 94.02%\n",
      "Epoch 9 \t Train loss(on avg per batch): 0.348713 \t Train Accuracy: 94.78% \t Validation loss(on avg per batch): 0.332476 \t Validation Accuracy: 94.9%\n",
      "Epoch 10 \t Train loss(on avg per batch): 0.296263 \t Train Accuracy: 95.265% \t Validation loss(on avg per batch): 0.285837 \t Validation Accuracy: 95.35%\n",
      "Best Performance on Validation data achived till now : 95.35\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set all initial parameters to random values\n",
    "bias_input = torch.rand(1)\n",
    "bias2 = torch.rand(1)\n",
    "bias3 = torch.rand(1)\n",
    "bias4 = torch.rand(1)\n",
    "bias5 = torch.rand(1)\n",
    "w1 = q1_net.fp_input.weight\n",
    "w2 = q1_net.fp1.weight\n",
    "w3 = q1_net.fp2.weight\n",
    "w4 = q1_net.fp3.weight\n",
    "w5 = q1_net.fp4.weight\n",
    "w_input_hat, w2_hat, w3_hat, w4_hat, w5_hat = get_low_rank_weights(w1, w2, w3, w4, w5) # get the weights from baseline model\n",
    "\n",
    "\n",
    "net = None\n",
    "net = low_rank_nn_q3( w_input_hat, bias_input, w2_hat, bias2, w3_hat, bias3, w4_hat, bias4, w5_hat, bias5)\n",
    "if(torch.cuda.is_available()):\n",
    "  net = net.float().to(device)\n",
    "else:\n",
    "  net = net.float()\n",
    "loss_func = nn.CrossEntropyLoss() # Here Sparse CLE is correct, but in pytorch nn.CrossEntropyLoss accepts ground truth labels directly as integers in [0, N_CLASSES] (no need to onehot encode the labels):\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = 0.001)\n",
    "print(net)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "q3_net_baseline, q3_weights_baseline, train_accuracy_all_baseline, valid_accuracy_all_baseline = run_custom_network(net, w_input_hat, bias_input, w2_hat, bias2, w3_hat, bias3, w4_hat, bias4, w5_hat, bias5, epochs = 10, loss_criteria = loss_func, optimizer = optimizer, learning_rate = 0.001, validation_process=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8G5PBEjTsdms"
   },
   "source": [
    "#### We see here although the accuracy is not as high as using the original matrix, this method of network compression is effective in reducing memory usage and also just a little drop in performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "n_U_UyIOzCRU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "Bem12v59zCV_"
   },
   "outputs": [],
   "source": [
    "# torch.save(q3_weights_baseline, '/content/drive/My Drive/Projects and research stuffs/DLS Assignments/Assignment 4/Q3 Weights')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jvdxGTESzCX7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__Nqiwa8fx2T"
   },
   "source": [
    "## Problem 4: Speaker Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "VtG-95bPzCZs"
   },
   "outputs": [],
   "source": [
    "with open(folder_path + 'hw4_trs.pkl', 'rb') as f:\n",
    "  x_train = pickle.load(f)\n",
    "\n",
    "with open(folder_path + 'hw4_tes.pkl', 'rb') as f:\n",
    "    x_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iQhEJhyWsjZ0",
    "outputId": "dbf123d0-7ac4-45d4-ffe7-9e48e100e3b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training speaker audio loaded (500, 16180)\n",
      "Test speaker audio loaded (200, 22631)\n"
     ]
    }
   ],
   "source": [
    "print('Training speaker audio loaded', x_train.shape)\n",
    "print('Test speaker audio loaded', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "MR5Ss28kssR3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NqIO4bsizCbk",
    "outputId": "b3a4277f-c03e-4aea-e617-f36a5c4ea67b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 50, 513)\n",
      "(200, 50, 513)\n",
      "(500, 1)\n",
      "(200, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_train_stft = np.zeros(shape=(500,50,513)) \n",
    "# Maximum length allowed is 50 (there is 45 length audio in test, so pad there with and train with extra zeroes)\n",
    "for i in range(x_train.shape[0]):\n",
    "    x = x_train[i,:]\n",
    "    X_complex = librosa.stft(x, n_fft=1024, hop_length=512)\n",
    "    X = np.abs(X_complex).T\n",
    "    x_train_stft[i,:,:] = np.pad(X,((0,50 - X.shape[0]),(0,0)), 'constant')\n",
    "print(x_train_stft.shape)\n",
    "\n",
    "x_test_stft = np.zeros(shape=(200,50,513))\n",
    "for i in range(x_test.shape[0]):\n",
    "  x = x_test[i,:]\n",
    "  X_complex = librosa.stft(x, n_fft=1024, hop_length=512)\n",
    "  X = np.abs(X_complex).T\n",
    "  x_test_stft[i,:,:] = np.pad(X,((0,50 - X.shape[0]),(0,0)), 'constant')\n",
    "print(x_test_stft.shape)\n",
    "\n",
    "# Preprocess target data\n",
    "y_train = np.zeros((500,1))\n",
    "count=0\n",
    "for i in range(0,50): # 50 speakers in train data, take each of them\n",
    "  for j in range(0,10): # Each with 10 utterences\n",
    "    y_train[count]=i\n",
    "    count+=1\n",
    "print(y_train.shape)\n",
    "\n",
    "y_test = np.zeros((200,1))\n",
    "count=0\n",
    "for i in range(0,20): # Each of 20 speakers in test data\n",
    "  for j in range(0,10): # Each with 10 utterences\n",
    "    y_test[count]=i\n",
    "    count+=1\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RkOTP8RJzCd5",
    "outputId": "42fce8ab-be6f-424f-c24a-7179ea55107e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4500, 2, 50, 513)\n",
      "(4500,)\n"
     ]
    }
   ],
   "source": [
    "positive_samples = 45\n",
    "negative_samples = 45\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "x_idx = np.arange(0,len(y_train)).reshape(-1,1)\n",
    "speaker_count_train = np.unique(y_train)\n",
    "train_pairs = []\n",
    "train_labels = []\n",
    "for speaker in speaker_count_train:\n",
    "  spk_idx = x_idx[np.where(y_train==speaker)] # Get all index where speaker is i\n",
    "  rest_spk_idx = x_idx[np.where(y_train!=speaker)]\n",
    "\n",
    "  pos_pairs = random.sample(list(combinations(spk_idx,2)),positive_samples) # WITHOUT REPALCEMENT\n",
    "  for pair in pos_pairs:\n",
    "    audio_stft_1 = x_train_stft[pair[0],:,:]\n",
    "    audio_stft_2 = x_train_stft[pair[1],:,:]\n",
    "    train_pairs.append( [audio_stft_1,audio_stft_2] )\n",
    "    train_labels.append(1)\n",
    "\n",
    "  neg_pairs = random.sample(list(product(spk_idx,rest_spk_idx)),negative_samples) # WITHOUT REPALCEMENT\n",
    "  for pair in neg_pairs:\n",
    "    audio_stft_1 = x_train_stft[pair[0],:,:]\n",
    "    audio_stft_2 = x_train_stft[pair[1],:,:]\n",
    "    train_pairs.append([audio_stft_1,audio_stft_2])\n",
    "    train_labels.append(0)    \n",
    "\n",
    "train_pairs, train_labels = np.array(train_pairs).astype(\"float\"), np.array(train_labels).astype(\"float\")\n",
    "    \n",
    "print(train_pairs.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jI_79GU2M1Ez",
    "outputId": "debedb09-d07f-4a81-8922-7c701335b52a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 50, 513)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4500 data, each is a pair of two vectors and each of those vecotrs is 50, 513. While bassing in batches, we can pass 128, 2, 50, 513\n",
    "train_pairs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-U37tNZqzChw",
    "outputId": "dd007a59-680b-4d7d-f902-48d2b7586383"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 2, 50, 513)\n",
      "(1800,)\n"
     ]
    }
   ],
   "source": [
    "positive_samples = 45\n",
    "negative_samples = 45\n",
    "\n",
    "y_test = np.array(y_test)\n",
    "x_idx = np.arange(0,len(y_test)).reshape(-1,1)\n",
    "speaker_count_test = np.unique(y_test)\n",
    "test_pairs = []\n",
    "test_labels = []\n",
    "for speaker in speaker_count_test:\n",
    "  spk_idx = x_idx[np.where(y_test==speaker)] # Get all index where speaker is i\n",
    "  rest_spk_idx = x_idx[np.where(y_test!=speaker)]\n",
    "\n",
    "  pos_pairs = random.sample(list(combinations(spk_idx,2)),positive_samples) # WITHOUT REPALCEMENT\n",
    "  for pair in pos_pairs:\n",
    "    audio_stft_1 = x_test_stft[pair[0],:,:]\n",
    "    audio_stft_2 = x_test_stft[pair[1],:,:]\n",
    "    test_pairs.append( [audio_stft_1,audio_stft_2] )\n",
    "    test_labels.append(1)\n",
    "\n",
    "  neg_pairs = random.sample(list(product(spk_idx,rest_spk_idx)),negative_samples) # WITHOUT REPALCEMENT\n",
    "  for pair in neg_pairs:\n",
    "    audio_stft_1 = x_test_stft[pair[0],:,:]\n",
    "    audio_stft_2 = x_test_stft[pair[1],:,:]\n",
    "    test_pairs.append([audio_stft_1,audio_stft_2])\n",
    "    test_labels.append(0)    \n",
    "\n",
    "test_pairs,test_labels = np.array(test_pairs).astype(\"float\"), np.array(test_labels).astype(\"float\")\n",
    "    \n",
    "print(test_pairs.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "dfVkPg1AJnjD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "KaoR2cpPCeOr"
   },
   "outputs": [],
   "source": [
    "# Dataloader creation\n",
    "batch_size_train = 90 # batch size train set\n",
    "batch_size_test = 90 # batch size test set\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_pairs), torch.tensor(train_labels) )\n",
    "valid_dataset = torch.utils.data.TensorDataset(torch.tensor(test_pairs), torch.tensor(test_labels) )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                             batch_size=batch_size_train, \n",
    "                                             shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, \n",
    "                                             batch_size=batch_size_test, \n",
    "                                             shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "P6FrWjyOzCj8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "FBpTIu3czCmL"
   },
   "outputs": [],
   "source": [
    "class siamese_q3(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # self.fp_input = nn.Linear(28*28, 1024)\n",
    "        self.gru1 = nn.GRU(input_size = 513, hidden_size = 256, num_layers = 2, bidirectional=True) # bidirectional=True sets both ways for parsing\n",
    "        self.fp1 = nn.Linear( int(512*50), 512 ) # 2 for bidirectional=True\n",
    "        self.fp2 = nn.Linear( 512, 256 )\n",
    "        self.fp_output = nn.Linear(90, 1)\n",
    "        self.ReLU = nn.ReLU()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        to_save_layerop = {}\n",
    "        x1 = input[:, 0, :, :]\n",
    "        x2 = input[:, 1, :, :]\n",
    "\n",
    "        # Simaese network with two audio spectra processed parallelly\n",
    "        input1, _ = self.gru1(x1)\n",
    "        input1 = torch.flatten(input1, start_dim = 1)        \n",
    "        input1 = self.fp1(input1)\n",
    "        input1 = self.ReLU(input1)\n",
    "        input1 = self.fp2(input1)\n",
    "        input1 = self.ReLU(input1)\n",
    "\n",
    "        input2, _ = self.gru1(x2) # Since we use shared weights for gru, both inputs have same gru\n",
    "        input2 = torch.flatten(input2, start_dim = 1)        \n",
    "        input2 = self.fp1(input2)\n",
    "        input2 = self.ReLU(input2)\n",
    "        input2 = self.fp2(input2)\n",
    "        input2 = self.ReLU(input2)\n",
    "\n",
    "        # Inner product of these two vectors will be used for binary-classification\n",
    "        # inner_prod = torch.matmul(input1, input2.T)\n",
    "        inner_prod = torch.cdist(input1, input2, p = 2)\n",
    "        # print(ed.shape)\n",
    "\n",
    "        input = self.fp_output(inner_prod)\n",
    "        # to_save_layerop['fp_output_before_softmax'] = input\n",
    "\n",
    "        # MOST IMPORTANT REALIZATION...USE SOFTMAX AFTER CALCULATING CCE LOSS, THE CCE LOSS EXPECTS LOGITS AND NOT SOFTMAX OUTPUTS.\n",
    "        return input, to_save_layerop    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "moAtWjKKzCoZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "-yHc5L1hzCqF"
   },
   "outputs": [],
   "source": [
    "def run_network(net, epochs, loss_criteria, optimizer, validation_process=True):\n",
    "    # A minor difference is that the implementation of CrossEntrypyLoss implicitly applies a softmax activation followed by a log transformation \n",
    "    # but NLLLoss does not.\n",
    "\n",
    "    # To keep best performance value\n",
    "    max_performance = float('-inf')\n",
    "    \n",
    "    # For early stopping\n",
    "    tolerance_level = 0\n",
    "    early_stopping_activated = 0\n",
    "    epoch = 0\n",
    "    MAX_MODEL = None\n",
    "    MAX_PERFORMANCE_WEIGHTS = None\n",
    "\n",
    "    train_loss_all = []\n",
    "    valid_loss_all = []\n",
    "    train_accuracy_all = []\n",
    "    valid_accuracy_all = []\n",
    "\n",
    "    print(len(train_loader))\n",
    "\n",
    "    while( epoch <= epochs and early_stopping_activated == 0 ):\n",
    "        train_loss = 0\n",
    "        valid_loss = 0\n",
    "        train_acc = 0\n",
    "        val_acc = 0\n",
    "        \n",
    "        # Training Part\n",
    "        # Always have this line to ensure proper training\n",
    "        net.train()\n",
    "        actual_labels_train = []\n",
    "        pred_labels_train = []\n",
    "        all_targets = []\n",
    "        all_preds = []\n",
    "\n",
    "        for i, (data, actual) in enumerate(train_loader):\n",
    "            # Push all variables to cuda\n",
    "            if(torch.cuda.is_available()):\n",
    "                data, actual =  data.float().to(device), actual.to(device)\n",
    "\n",
    "            output, _ = net(data.float())\n",
    "            actual = actual.reshape(-1,1)\n",
    "            loss =  loss_criteria(output, actual)\n",
    "            # Track loss\n",
    "            train_loss += loss.item()\n",
    "            train_predictions = torch.sigmoid(output)\n",
    "            train_predictions = (train_predictions>0.5).float()*1  \n",
    "            actual_labels_train.append(actual.cpu().numpy())\n",
    "            pred_labels_train.append(train_predictions.cpu().numpy())\n",
    "\n",
    "            optimizer.zero_grad() # reset gradients\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        all_targets = np.concatenate(actual_labels_train, axis=0)\n",
    "        all_preds = np.concatenate(pred_labels_train, axis=0)\n",
    "        train_acc = accuracy_score(all_preds, all_targets)\n",
    "        train_accuracy_all.append(train_acc)\n",
    "\n",
    "\n",
    "        # Evaluation Part\n",
    "        # Always have this line to ensure proper evaluation \n",
    "        if(validation_process):\n",
    "          net.eval()\n",
    "          \n",
    "          actual_labels_all = []\n",
    "          pred_labels_all = []\n",
    "          all_targets = []\n",
    "          all_preds = []\n",
    "          # Now do validation and keep track of valid loss\n",
    "          with torch.no_grad():\n",
    "              for i, (data, labels) in enumerate(valid_loader):\n",
    "                  if(torch.cuda.is_available()):\n",
    "                      data, labels = data.float().to(device), labels.to(device)\n",
    "\n",
    "                  # FP     \n",
    "                  val_logits, _ = net.forward(data)\n",
    "                  labels = labels.reshape(-1,1)\n",
    "                  v_loss = loss_criteria(val_logits, labels)\n",
    "                  valid_loss += v_loss.item()\n",
    "                  val_predictions = torch.sigmoid(val_logits)\n",
    "                  val_predictions = (val_predictions>0.5).float()*1 \n",
    "                  actual_labels_all.append(labels.cpu().numpy())\n",
    "                  pred_labels_all.append(val_predictions.cpu().numpy())\n",
    "                  \n",
    "          all_targets = np.concatenate(actual_labels_all, axis=0)\n",
    "          all_preds = np.concatenate(pred_labels_all, axis=0)\n",
    "          val_acc = accuracy_score(all_preds, all_targets)\n",
    "          valid_accuracy_all.append(val_acc)\n",
    "          \n",
    "          if(epoch % 1 == 0):\n",
    "            print(f'Epoch {epoch} \\t Train loss(on avg per batch): {round(train_loss/len(train_loader), 6) } \\t Train Accuracy: {round(train_acc*100, 6)}% \\t Validation loss(on avg per batch): {round(valid_loss/len(valid_loader),6)} \\t Validation Accuracy: {round(val_acc*100,6)}%')\n",
    "\n",
    "          train_loss_all.append(train_loss/len(train_loader))\n",
    "          valid_loss_all.append(valid_loss/len(valid_loader)) \n",
    "          \n",
    "          if(val_acc*100 > max_performance):\n",
    "              max_performance = val_acc*100\n",
    "              MAX_PERFORMANCE_WEIGHTS = net.state_dict()\n",
    "              MAX_MODEL = net\n",
    "              \n",
    "          # else:\n",
    "          #     tolerance_level+=1\n",
    "          #     if(tolerance_level >= 10):\n",
    "          #         early_stopping_activated = 1\n",
    "          #         print('Early Stopping activated - no improvement in validation accuracy for the past 10 epochs. Using model stage before 10 epochs for further use!')\n",
    "        epoch+=1\n",
    "\n",
    "    if(not validation_process):\n",
    "        # Save Model at end\n",
    "        MAX_PERFORMANCE_WEIGHTS = net.state_dict()\n",
    "        MAX_MODEL = net\n",
    "\n",
    "    print('Best Performance on Validation data achived till now :', max_performance)\n",
    "    \n",
    "    return MAX_MODEL, MAX_PERFORMANCE_WEIGHTS, train_accuracy_all, valid_accuracy_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17lOAH7gzCsP",
    "outputId": "22d768a5-4ab0-42d9-a65e-62912dc05f07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "siamese_q3(\n",
      "  (gru1): GRU(513, 256, num_layers=2, bidirectional=True)\n",
      "  (fp1): Linear(in_features=25600, out_features=512, bias=True)\n",
      "  (fp2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fp_output): Linear(in_features=90, out_features=1, bias=True)\n",
      "  (ReLU): ReLU()\n",
      ")\n",
      "50\n",
      "Epoch 0 \t Train loss(on avg per batch): 0.715402 \t Train Accuracy: 48.711111% \t Validation loss(on avg per batch): 0.756884 \t Validation Accuracy: 52.388889%\n",
      "Epoch 1 \t Train loss(on avg per batch): 0.701141 \t Train Accuracy: 49.533333% \t Validation loss(on avg per batch): 0.689233 \t Validation Accuracy: 51.5%\n",
      "Epoch 2 \t Train loss(on avg per batch): 0.699673 \t Train Accuracy: 49.266667% \t Validation loss(on avg per batch): 0.687278 \t Validation Accuracy: 55.333333%\n",
      "Epoch 3 \t Train loss(on avg per batch): 0.694773 \t Train Accuracy: 50.088889% \t Validation loss(on avg per batch): 0.692849 \t Validation Accuracy: 52.222222%\n",
      "Epoch 4 \t Train loss(on avg per batch): 0.693772 \t Train Accuracy: 50.266667% \t Validation loss(on avg per batch): 0.694413 \t Validation Accuracy: 44.055556%\n",
      "Epoch 5 \t Train loss(on avg per batch): 0.694096 \t Train Accuracy: 50.511111% \t Validation loss(on avg per batch): 0.69113 \t Validation Accuracy: 56.277778%\n",
      "Epoch 6 \t Train loss(on avg per batch): 0.695564 \t Train Accuracy: 50.466667% \t Validation loss(on avg per batch): 0.697516 \t Validation Accuracy: 50.0%\n",
      "Epoch 7 \t Train loss(on avg per batch): 0.698461 \t Train Accuracy: 49.688889% \t Validation loss(on avg per batch): 0.688938 \t Validation Accuracy: 50.333333%\n",
      "Epoch 8 \t Train loss(on avg per batch): 0.701605 \t Train Accuracy: 48.733333% \t Validation loss(on avg per batch): 0.690989 \t Validation Accuracy: 47.111111%\n",
      "Epoch 9 \t Train loss(on avg per batch): 0.693539 \t Train Accuracy: 50.377778% \t Validation loss(on avg per batch): 0.692722 \t Validation Accuracy: 45.833333%\n",
      "Epoch 10 \t Train loss(on avg per batch): 0.693235 \t Train Accuracy: 51.222222% \t Validation loss(on avg per batch): 0.695901 \t Validation Accuracy: 48.444444%\n",
      "Epoch 11 \t Train loss(on avg per batch): 0.693548 \t Train Accuracy: 47.955556% \t Validation loss(on avg per batch): 0.690971 \t Validation Accuracy: 51.722222%\n",
      "Epoch 12 \t Train loss(on avg per batch): 0.693558 \t Train Accuracy: 50.422222% \t Validation loss(on avg per batch): 0.697086 \t Validation Accuracy: 35.333333%\n",
      "Epoch 13 \t Train loss(on avg per batch): 0.693525 \t Train Accuracy: 48.377778% \t Validation loss(on avg per batch): 0.693478 \t Validation Accuracy: 50.055556%\n",
      "Epoch 14 \t Train loss(on avg per batch): 0.693366 \t Train Accuracy: 49.755556% \t Validation loss(on avg per batch): 0.693322 \t Validation Accuracy: 47.944444%\n",
      "Epoch 15 \t Train loss(on avg per batch): 0.693322 \t Train Accuracy: 48.644444% \t Validation loss(on avg per batch): 0.693754 \t Validation Accuracy: 47.388889%\n",
      "Epoch 16 \t Train loss(on avg per batch): 0.693333 \t Train Accuracy: 49.577778% \t Validation loss(on avg per batch): 0.69341 \t Validation Accuracy: 49.333333%\n",
      "Epoch 17 \t Train loss(on avg per batch): 0.693362 \t Train Accuracy: 49.711111% \t Validation loss(on avg per batch): 0.69337 \t Validation Accuracy: 49.111111%\n",
      "Epoch 18 \t Train loss(on avg per batch): 0.693155 \t Train Accuracy: 50.422222% \t Validation loss(on avg per batch): 0.695257 \t Validation Accuracy: 49.166667%\n",
      "Epoch 19 \t Train loss(on avg per batch): 0.693731 \t Train Accuracy: 49.488889% \t Validation loss(on avg per batch): 0.693123 \t Validation Accuracy: 48.888889%\n",
      "Epoch 20 \t Train loss(on avg per batch): 0.692887 \t Train Accuracy: 51.777778% \t Validation loss(on avg per batch): 0.693003 \t Validation Accuracy: 50.055556%\n",
      "Epoch 21 \t Train loss(on avg per batch): 0.693238 \t Train Accuracy: 50.355556% \t Validation loss(on avg per batch): 0.692356 \t Validation Accuracy: 52.777778%\n",
      "Epoch 22 \t Train loss(on avg per batch): 0.693423 \t Train Accuracy: 49.977778% \t Validation loss(on avg per batch): 0.692981 \t Validation Accuracy: 50.833333%\n",
      "Epoch 23 \t Train loss(on avg per batch): 0.693915 \t Train Accuracy: 49.288889% \t Validation loss(on avg per batch): 0.699334 \t Validation Accuracy: 32.055556%\n",
      "Epoch 24 \t Train loss(on avg per batch): 0.693282 \t Train Accuracy: 50.6% \t Validation loss(on avg per batch): 0.693362 \t Validation Accuracy: 49.611111%\n",
      "Epoch 25 \t Train loss(on avg per batch): 0.693716 \t Train Accuracy: 49.911111% \t Validation loss(on avg per batch): 0.693524 \t Validation Accuracy: 49.833333%\n",
      "Epoch 26 \t Train loss(on avg per batch): 0.693928 \t Train Accuracy: 49.044444% \t Validation loss(on avg per batch): 0.692132 \t Validation Accuracy: 51.555556%\n",
      "Epoch 27 \t Train loss(on avg per batch): 0.693181 \t Train Accuracy: 49.933333% \t Validation loss(on avg per batch): 0.693642 \t Validation Accuracy: 50.888889%\n",
      "Epoch 28 \t Train loss(on avg per batch): 0.693236 \t Train Accuracy: 50.0% \t Validation loss(on avg per batch): 0.692993 \t Validation Accuracy: 50.555556%\n",
      "Epoch 29 \t Train loss(on avg per batch): 0.693239 \t Train Accuracy: 49.777778% \t Validation loss(on avg per batch): 0.693751 \t Validation Accuracy: 47.777778%\n",
      "Epoch 30 \t Train loss(on avg per batch): 0.693264 \t Train Accuracy: 51.088889% \t Validation loss(on avg per batch): 0.693842 \t Validation Accuracy: 48.777778%\n",
      "Epoch 31 \t Train loss(on avg per batch): 0.693301 \t Train Accuracy: 50.622222% \t Validation loss(on avg per batch): 0.693564 \t Validation Accuracy: 50.222222%\n",
      "Epoch 32 \t Train loss(on avg per batch): 0.693717 \t Train Accuracy: 49.466667% \t Validation loss(on avg per batch): 0.69427 \t Validation Accuracy: 48.888889%\n",
      "Epoch 33 \t Train loss(on avg per batch): 0.693882 \t Train Accuracy: 48.711111% \t Validation loss(on avg per batch): 0.693275 \t Validation Accuracy: 49.722222%\n",
      "Epoch 34 \t Train loss(on avg per batch): 0.693595 \t Train Accuracy: 49.533333% \t Validation loss(on avg per batch): 0.693741 \t Validation Accuracy: 47.888889%\n",
      "Epoch 35 \t Train loss(on avg per batch): 0.693439 \t Train Accuracy: 49.666667% \t Validation loss(on avg per batch): 0.693837 \t Validation Accuracy: 49.222222%\n",
      "Epoch 36 \t Train loss(on avg per batch): 0.693859 \t Train Accuracy: 48.644444% \t Validation loss(on avg per batch): 0.692528 \t Validation Accuracy: 50.777778%\n",
      "Epoch 37 \t Train loss(on avg per batch): 0.693996 \t Train Accuracy: 49.466667% \t Validation loss(on avg per batch): 0.69374 \t Validation Accuracy: 50.111111%\n",
      "Epoch 38 \t Train loss(on avg per batch): 0.69414 \t Train Accuracy: 49.177778% \t Validation loss(on avg per batch): 0.693571 \t Validation Accuracy: 50.444444%\n",
      "Epoch 39 \t Train loss(on avg per batch): 0.694001 \t Train Accuracy: 49.977778% \t Validation loss(on avg per batch): 0.69482 \t Validation Accuracy: 48.611111%\n",
      "Epoch 40 \t Train loss(on avg per batch): 0.693897 \t Train Accuracy: 48.844444% \t Validation loss(on avg per batch): 0.692537 \t Validation Accuracy: 51.055556%\n",
      "Epoch 41 \t Train loss(on avg per batch): 0.692778 \t Train Accuracy: 50.688889% \t Validation loss(on avg per batch): 0.693643 \t Validation Accuracy: 49.0%\n",
      "Epoch 42 \t Train loss(on avg per batch): 0.693191 \t Train Accuracy: 50.533333% \t Validation loss(on avg per batch): 0.69321 \t Validation Accuracy: 50.222222%\n",
      "Epoch 43 \t Train loss(on avg per batch): 0.693741 \t Train Accuracy: 48.622222% \t Validation loss(on avg per batch): 0.693359 \t Validation Accuracy: 50.0%\n",
      "Epoch 44 \t Train loss(on avg per batch): 0.693703 \t Train Accuracy: 48.644444% \t Validation loss(on avg per batch): 0.693297 \t Validation Accuracy: 50.166667%\n",
      "Epoch 45 \t Train loss(on avg per batch): 0.693163 \t Train Accuracy: 50.355556% \t Validation loss(on avg per batch): 0.692945 \t Validation Accuracy: 50.666667%\n",
      "Epoch 46 \t Train loss(on avg per batch): 0.693291 \t Train Accuracy: 50.0% \t Validation loss(on avg per batch): 0.693128 \t Validation Accuracy: 48.222222%\n",
      "Epoch 47 \t Train loss(on avg per batch): 0.693438 \t Train Accuracy: 49.222222% \t Validation loss(on avg per batch): 0.693206 \t Validation Accuracy: 50.0%\n",
      "Epoch 48 \t Train loss(on avg per batch): 0.693138 \t Train Accuracy: 49.844444% \t Validation loss(on avg per batch): 0.693147 \t Validation Accuracy: 50.5%\n",
      "Epoch 49 \t Train loss(on avg per batch): 0.693334 \t Train Accuracy: 49.088889% \t Validation loss(on avg per batch): 0.693742 \t Validation Accuracy: 48.166667%\n",
      "Epoch 50 \t Train loss(on avg per batch): 0.693307 \t Train Accuracy: 49.555556% \t Validation loss(on avg per batch): 0.693582 \t Validation Accuracy: 50.111111%\n",
      "Epoch 51 \t Train loss(on avg per batch): 0.693396 \t Train Accuracy: 48.844444% \t Validation loss(on avg per batch): 0.692666 \t Validation Accuracy: 51.444444%\n",
      "Epoch 52 \t Train loss(on avg per batch): 0.693328 \t Train Accuracy: 49.333333% \t Validation loss(on avg per batch): 0.693403 \t Validation Accuracy: 47.555556%\n",
      "Epoch 53 \t Train loss(on avg per batch): 0.69371 \t Train Accuracy: 47.888889% \t Validation loss(on avg per batch): 0.693449 \t Validation Accuracy: 47.222222%\n",
      "Epoch 54 \t Train loss(on avg per batch): 0.693037 \t Train Accuracy: 50.044444% \t Validation loss(on avg per batch): 0.693766 \t Validation Accuracy: 48.888889%\n",
      "Epoch 55 \t Train loss(on avg per batch): 0.693435 \t Train Accuracy: 48.622222% \t Validation loss(on avg per batch): 0.693116 \t Validation Accuracy: 51.777778%\n",
      "Epoch 56 \t Train loss(on avg per batch): 0.6933 \t Train Accuracy: 49.355556% \t Validation loss(on avg per batch): 0.693157 \t Validation Accuracy: 49.444444%\n",
      "Epoch 57 \t Train loss(on avg per batch): 0.693371 \t Train Accuracy: 48.688889% \t Validation loss(on avg per batch): 0.692896 \t Validation Accuracy: 49.777778%\n",
      "Epoch 58 \t Train loss(on avg per batch): 0.692841 \t Train Accuracy: 50.755556% \t Validation loss(on avg per batch): 0.692907 \t Validation Accuracy: 51.333333%\n",
      "Epoch 59 \t Train loss(on avg per batch): 0.69301 \t Train Accuracy: 50.622222% \t Validation loss(on avg per batch): 0.691827 \t Validation Accuracy: 52.611111%\n",
      "Epoch 60 \t Train loss(on avg per batch): 0.693329 \t Train Accuracy: 49.488889% \t Validation loss(on avg per batch): 0.691748 \t Validation Accuracy: 54.166667%\n",
      "Epoch 61 \t Train loss(on avg per batch): 0.693324 \t Train Accuracy: 49.777778% \t Validation loss(on avg per batch): 0.6929 \t Validation Accuracy: 51.333333%\n",
      "Epoch 62 \t Train loss(on avg per batch): 0.693159 \t Train Accuracy: 49.8% \t Validation loss(on avg per batch): 0.691266 \t Validation Accuracy: 53.388889%\n",
      "Epoch 63 \t Train loss(on avg per batch): 0.694 \t Train Accuracy: 49.511111% \t Validation loss(on avg per batch): 0.692687 \t Validation Accuracy: 52.666667%\n",
      "Epoch 64 \t Train loss(on avg per batch): 0.693792 \t Train Accuracy: 48.377778% \t Validation loss(on avg per batch): 0.695144 \t Validation Accuracy: 47.388889%\n",
      "Epoch 65 \t Train loss(on avg per batch): 0.69359 \t Train Accuracy: 49.777778% \t Validation loss(on avg per batch): 0.692833 \t Validation Accuracy: 52.444444%\n",
      "Epoch 66 \t Train loss(on avg per batch): 0.692833 \t Train Accuracy: 51.155556% \t Validation loss(on avg per batch): 0.693767 \t Validation Accuracy: 49.722222%\n",
      "Epoch 67 \t Train loss(on avg per batch): 0.69386 \t Train Accuracy: 49.266667% \t Validation loss(on avg per batch): 0.692319 \t Validation Accuracy: 52.222222%\n",
      "Epoch 68 \t Train loss(on avg per batch): 0.693683 \t Train Accuracy: 49.222222% \t Validation loss(on avg per batch): 0.692101 \t Validation Accuracy: 51.722222%\n",
      "Epoch 69 \t Train loss(on avg per batch): 0.693875 \t Train Accuracy: 50.066667% \t Validation loss(on avg per batch): 0.693481 \t Validation Accuracy: 49.833333%\n",
      "Epoch 70 \t Train loss(on avg per batch): 0.69361 \t Train Accuracy: 50.333333% \t Validation loss(on avg per batch): 0.697111 \t Validation Accuracy: 49.277778%\n",
      "Epoch 71 \t Train loss(on avg per batch): 0.695254 \t Train Accuracy: 49.044444% \t Validation loss(on avg per batch): 0.697184 \t Validation Accuracy: 47.611111%\n",
      "Epoch 72 \t Train loss(on avg per batch): 0.694482 \t Train Accuracy: 49.711111% \t Validation loss(on avg per batch): 0.697614 \t Validation Accuracy: 46.166667%\n",
      "Epoch 73 \t Train loss(on avg per batch): 0.694747 \t Train Accuracy: 47.955556% \t Validation loss(on avg per batch): 0.694654 \t Validation Accuracy: 46.166667%\n",
      "Epoch 74 \t Train loss(on avg per batch): 0.694089 \t Train Accuracy: 49.911111% \t Validation loss(on avg per batch): 0.694592 \t Validation Accuracy: 49.0%\n",
      "Epoch 75 \t Train loss(on avg per batch): 0.69367 \t Train Accuracy: 49.911111% \t Validation loss(on avg per batch): 0.693449 \t Validation Accuracy: 50.666667%\n",
      "Epoch 76 \t Train loss(on avg per batch): 0.693519 \t Train Accuracy: 50.0% \t Validation loss(on avg per batch): 0.692176 \t Validation Accuracy: 51.0%\n",
      "Epoch 77 \t Train loss(on avg per batch): 0.694783 \t Train Accuracy: 49.911111% \t Validation loss(on avg per batch): 0.69574 \t Validation Accuracy: 50.277778%\n",
      "Epoch 78 \t Train loss(on avg per batch): 0.694129 \t Train Accuracy: 49.755556% \t Validation loss(on avg per batch): 0.696011 \t Validation Accuracy: 50.166667%\n",
      "Epoch 79 \t Train loss(on avg per batch): 0.692948 \t Train Accuracy: 50.244444% \t Validation loss(on avg per batch): 0.694697 \t Validation Accuracy: 48.666667%\n",
      "Epoch 80 \t Train loss(on avg per batch): 0.695882 \t Train Accuracy: 48.844444% \t Validation loss(on avg per batch): 0.691315 \t Validation Accuracy: 53.5%\n",
      "Epoch 81 \t Train loss(on avg per batch): 0.692785 \t Train Accuracy: 50.711111% \t Validation loss(on avg per batch): 0.691862 \t Validation Accuracy: 53.111111%\n",
      "Epoch 82 \t Train loss(on avg per batch): 0.69377 \t Train Accuracy: 50.088889% \t Validation loss(on avg per batch): 0.694278 \t Validation Accuracy: 49.333333%\n",
      "Epoch 83 \t Train loss(on avg per batch): 0.693844 \t Train Accuracy: 48.888889% \t Validation loss(on avg per batch): 0.693705 \t Validation Accuracy: 51.5%\n",
      "Epoch 84 \t Train loss(on avg per batch): 0.694115 \t Train Accuracy: 48.955556% \t Validation loss(on avg per batch): 0.695166 \t Validation Accuracy: 46.833333%\n",
      "Epoch 85 \t Train loss(on avg per batch): 0.693245 \t Train Accuracy: 50.088889% \t Validation loss(on avg per batch): 0.693834 \t Validation Accuracy: 48.833333%\n",
      "Epoch 86 \t Train loss(on avg per batch): 0.693196 \t Train Accuracy: 50.155556% \t Validation loss(on avg per batch): 0.692829 \t Validation Accuracy: 51.833333%\n",
      "Epoch 87 \t Train loss(on avg per batch): 0.693138 \t Train Accuracy: 50.288889% \t Validation loss(on avg per batch): 0.694335 \t Validation Accuracy: 47.611111%\n",
      "Epoch 88 \t Train loss(on avg per batch): 0.693654 \t Train Accuracy: 49.866667% \t Validation loss(on avg per batch): 0.693642 \t Validation Accuracy: 48.388889%\n",
      "Epoch 89 \t Train loss(on avg per batch): 0.693349 \t Train Accuracy: 50.511111% \t Validation loss(on avg per batch): 0.694287 \t Validation Accuracy: 48.444444%\n",
      "Epoch 90 \t Train loss(on avg per batch): 0.692861 \t Train Accuracy: 50.644444% \t Validation loss(on avg per batch): 0.694164 \t Validation Accuracy: 47.5%\n",
      "Epoch 91 \t Train loss(on avg per batch): 0.693391 \t Train Accuracy: 49.4% \t Validation loss(on avg per batch): 0.693638 \t Validation Accuracy: 48.777778%\n",
      "Epoch 92 \t Train loss(on avg per batch): 0.693342 \t Train Accuracy: 49.6% \t Validation loss(on avg per batch): 0.693635 \t Validation Accuracy: 48.444444%\n",
      "Epoch 93 \t Train loss(on avg per batch): 0.69351 \t Train Accuracy: 49.866667% \t Validation loss(on avg per batch): 0.692923 \t Validation Accuracy: 51.388889%\n",
      "Epoch 94 \t Train loss(on avg per batch): 0.693244 \t Train Accuracy: 49.644444% \t Validation loss(on avg per batch): 0.693257 \t Validation Accuracy: 49.0%\n",
      "Epoch 95 \t Train loss(on avg per batch): 0.693258 \t Train Accuracy: 49.888889% \t Validation loss(on avg per batch): 0.693151 \t Validation Accuracy: 50.111111%\n",
      "Epoch 96 \t Train loss(on avg per batch): 0.693312 \t Train Accuracy: 48.577778% \t Validation loss(on avg per batch): 0.692917 \t Validation Accuracy: 52.333333%\n",
      "Epoch 97 \t Train loss(on avg per batch): 0.693035 \t Train Accuracy: 50.288889% \t Validation loss(on avg per batch): 0.69289 \t Validation Accuracy: 52.666667%\n",
      "Epoch 98 \t Train loss(on avg per batch): 0.693287 \t Train Accuracy: 49.222222% \t Validation loss(on avg per batch): 0.693212 \t Validation Accuracy: 48.722222%\n",
      "Epoch 99 \t Train loss(on avg per batch): 0.693158 \t Train Accuracy: 49.8% \t Validation loss(on avg per batch): 0.693319 \t Validation Accuracy: 48.833333%\n",
      "Epoch 100 \t Train loss(on avg per batch): 0.69315 \t Train Accuracy: 50.688889% \t Validation loss(on avg per batch): 0.693224 \t Validation Accuracy: 47.722222%\n",
      "Best Performance on Validation data achived till now : 56.277777777777786\n"
     ]
    }
   ],
   "source": [
    "net = None\n",
    "net = siamese_q3()\n",
    "if(torch.cuda.is_available()):\n",
    "  net = net.float().to(device)\n",
    "else:\n",
    "  net = net.float()\n",
    "# net.apply(he_weights) # ContrastiveLoss # BCEWithLogitsLoss\n",
    "loss_func = nn.BCEWithLogitsLoss() # Here Sparse CLE is correct, but in pytorch nn.CrossEntropyLoss accepts ground truth labels directly as integers in [0, N_CLASSES] (no need to onehot encode the labels):\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = 0.001)\n",
    "print(net)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "q4_net_siamese, q4_weights_siamese, train_accuracy_all_siamese, valid_accuracy_all_siamese = run_network(net, epochs = 100, loss_criteria = loss_func, optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "If-bt2NOADSx"
   },
   "source": [
    "## I am still not sure why the accuracy won't go up, I am not sure if the model is getting trained properly - please let me know what other techniques I can use to increase the validation accuracy. The weights are getting updated, so that is not the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "OTY0ToE41J2v"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "Z2LKVTQJzCwI",
    "outputId": "ffac789b-1cbc-4513-aa37-02a01cc806fe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure(figsize=(10, 6), dpi=80)\n",
    "plt.plot(range(0, len(train_accuracy_all_siamese)), train_accuracy_all_siamese, color = 'red')\n",
    "plt.title('Test accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "I76MX1QdzCz1"
   },
   "outputs": [],
   "source": [
    "torch.save(q4_weights_siamese, '/content/drive/My Drive/Projects and research stuffs/DLS Assignments/Assignment 4/Q4 Weights Siamese')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lt9YLAWL0HMl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RkvThQZY0HPx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SC7B30Wd0HSX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "lanENqxk7S1-",
    "5AZ3Gin98XDq",
    "Rm0eUlcPAAK-",
    "5eF5cPqhe9jk",
    "IuW6EV85zGEK"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
