{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNekAJsvijaou3knrUFmdhO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ram130849/Deep_Learning_Systems_Assignments/blob/main/TensorFlow/Sushant/DLS_Assignment_3_Part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cA23lU8DwE_Z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import librosa\n",
        "import librosa.display\n",
        "import timeit\n",
        "from IPython.display import Audio\n",
        "from sklearn.model_selection import train_test_split\n",
        "import soundfile as sf\n",
        "from tensorflow.keras.layers import Conv1D,Conv2D,MaxPooling1D,MaxPooling2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from keras.models import Model\n",
        "import pickle\n",
        "import tarfile"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ug9N64kOMvf",
        "outputId": "f34975ff-9cfa-46c6-a6d6-8b9ba485b2bc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0nqdH5IONgc",
        "outputId": "93a36598-8aa6-4d4d-d8c8-4bb45ba66a5e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "3.2003898880000037\n",
            "GPU (s):\n",
            "0.04204798600000004\n",
            "GPU speedup over CPU: 76x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def unpickle(file):\n",
        "    #import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict"
      ],
      "metadata": {
        "id": "sUPe4ZZvwlC6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cifar10 = keras.datasets.cifar10"
      ],
      "metadata": {
        "id": "wE38io5XxG-X"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cifar10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09Kc9q8cxshI",
        "outputId": "f0360c06-0587-45eb-b465-68515929e759"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'keras.api._v2.keras.datasets.cifar10' from '/usr/local/lib/python3.7/dist-packages/keras/api/_v2/keras/datasets/cifar10/__init__.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
      ],
      "metadata": {
        "id": "gL8eh659zXlr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3550a7f9-da13-4de7-95ad-6caffb5d57e0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 14s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The shape of training set is : \",x_train.shape)\n",
        "print(\"The shape of training labels is : \",y_train.shape)\n",
        "print(\"The shape of testing set is : \",x_test.shape)\n",
        "print(\"The shape of testing labels is : \",y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MrV5En6zZrz",
        "outputId": "08e71a58-f318-4633-a2fa-722d5dde5944"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape of training set is :  (50000, 32, 32, 3)\n",
            "The shape of training labels is :  (50000, 1)\n",
            "The shape of testing set is :  (10000, 32, 32, 3)\n",
            "The shape of testing labels is :  (10000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The size of training set is : \",len(x_train))\n",
        "print(\"The size of training labels is : \",len(y_train))\n",
        "print(\"The size of testing set is : \",len(x_test))\n",
        "print(\"The size of testing labels is : \",len(y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQA60IcV2O6S",
        "outputId": "4fe0895c-ef47-438f-d784-e6df88ef3e60"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The size of training set is :  50000\n",
            "The size of training labels is :  50000\n",
            "The size of testing set is :  10000\n",
            "The size of testing labels is :  10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "234BWS2g3JF9",
        "outputId": "e73ec39c-204c-493b-e106-f2b53b46b298"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[ 59,  62,  63],\n",
              "        [ 43,  46,  45],\n",
              "        [ 50,  48,  43],\n",
              "        ...,\n",
              "        [158, 132, 108],\n",
              "        [152, 125, 102],\n",
              "        [148, 124, 103]],\n",
              "\n",
              "       [[ 16,  20,  20],\n",
              "        [  0,   0,   0],\n",
              "        [ 18,   8,   0],\n",
              "        ...,\n",
              "        [123,  88,  55],\n",
              "        [119,  83,  50],\n",
              "        [122,  87,  57]],\n",
              "\n",
              "       [[ 25,  24,  21],\n",
              "        [ 16,   7,   0],\n",
              "        [ 49,  27,   8],\n",
              "        ...,\n",
              "        [118,  84,  50],\n",
              "        [120,  84,  50],\n",
              "        [109,  73,  42]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[208, 170,  96],\n",
              "        [201, 153,  34],\n",
              "        [198, 161,  26],\n",
              "        ...,\n",
              "        [160, 133,  70],\n",
              "        [ 56,  31,   7],\n",
              "        [ 53,  34,  20]],\n",
              "\n",
              "       [[180, 139,  96],\n",
              "        [173, 123,  42],\n",
              "        [186, 144,  30],\n",
              "        ...,\n",
              "        [184, 148,  94],\n",
              "        [ 97,  62,  34],\n",
              "        [ 83,  53,  34]],\n",
              "\n",
              "       [[177, 144, 116],\n",
              "        [168, 129,  94],\n",
              "        [179, 142,  87],\n",
              "        ...,\n",
              "        [216, 184, 140],\n",
              "        [151, 118,  84],\n",
              "        [123,  92,  72]]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_val,Y_train,Y_val = train_test_split(x_train,y_train,test_size=0.1,random_state=6)"
      ],
      "metadata": {
        "id": "cyx5e6Hh5zkB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpsbccfpO_OU",
        "outputId": "b2ee6a69-6562-459e-890e-05380c0a598b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(45000, 32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mv0ios7gPCuh",
        "outputId": "a3c92899-b46e-459f-9821-fcedd2578771"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(45000, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnYmGsd8_ELY",
        "outputId": "3bffc47c-f00f-4966-d5c1-e0299261442e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[[163, 177, 156],\n",
              "         [ 91, 116,  85],\n",
              "         [ 92, 126,  87],\n",
              "         ...,\n",
              "         [112, 151, 100],\n",
              "         [ 77, 109,  72],\n",
              "         [134, 153, 130]],\n",
              "\n",
              "        [[100, 124,  86],\n",
              "         [128, 154, 119],\n",
              "         [172, 202, 167],\n",
              "         ...,\n",
              "         [157, 190, 145],\n",
              "         [155, 183, 155],\n",
              "         [ 94, 118,  95]],\n",
              "\n",
              "        [[101, 132,  86],\n",
              "         [154, 184, 142],\n",
              "         [ 83, 118,  68],\n",
              "         ...,\n",
              "         [ 62,  96,  39],\n",
              "         [133, 161, 127],\n",
              "         [139, 167, 136]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[119, 149, 114],\n",
              "         [135, 169, 132],\n",
              "         [ 68, 101,  38],\n",
              "         ...,\n",
              "         [ 52,  89,  18],\n",
              "         [ 94, 132,  76],\n",
              "         [151, 181, 145]],\n",
              "\n",
              "        [[ 90, 117,  83],\n",
              "         [154, 182, 150],\n",
              "         [130, 159, 119],\n",
              "         ...,\n",
              "         [ 90, 124,  77],\n",
              "         [156, 186, 146],\n",
              "         [111, 138, 104]],\n",
              "\n",
              "        [[130, 151, 121],\n",
              "         [ 96, 123,  90],\n",
              "         [156, 185, 152],\n",
              "         ...,\n",
              "         [173, 202, 170],\n",
              "         [113, 141, 107],\n",
              "         [103, 126,  95]]],\n",
              "\n",
              "\n",
              "       [[[173, 165, 153],\n",
              "         [179, 171, 159],\n",
              "         [183, 175, 163],\n",
              "         ...,\n",
              "         [ 63,  57,  57],\n",
              "         [ 15,  14,  20],\n",
              "         [  5,   8,   7]],\n",
              "\n",
              "        [[129, 119, 102],\n",
              "         [142, 132, 115],\n",
              "         [161, 151, 134],\n",
              "         ...,\n",
              "         [ 94,  88,  87],\n",
              "         [ 36,  35,  41],\n",
              "         [  6,   8,   8]],\n",
              "\n",
              "        [[ 98,  85,  62],\n",
              "         [ 93,  80,  57],\n",
              "         [110,  97,  74],\n",
              "         ...,\n",
              "         [ 76,  70,  70],\n",
              "         [ 24,  23,  30],\n",
              "         [  6,   8,   8]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 99,  86,  40],\n",
              "         [ 81,  69,  30],\n",
              "         [102,  85,  50],\n",
              "         ...,\n",
              "         [ 97,  88,  70],\n",
              "         [100,  98,  71],\n",
              "         [122, 139, 112]],\n",
              "\n",
              "        [[139, 128,  76],\n",
              "         [116, 108,  63],\n",
              "         [109,  97,  56],\n",
              "         ...,\n",
              "         [ 73,  76,  60],\n",
              "         [ 97, 112,  91],\n",
              "         [125, 152, 132]],\n",
              "\n",
              "        [[166, 154, 116],\n",
              "         [158, 152, 116],\n",
              "         [151, 142, 111],\n",
              "         ...,\n",
              "         [ 85,  99,  85],\n",
              "         [123, 149, 134],\n",
              "         [143, 178, 164]]],\n",
              "\n",
              "\n",
              "       [[[212, 185, 164],\n",
              "         [200, 176, 150],\n",
              "         [111,  99,  67],\n",
              "         ...,\n",
              "         [ 82,  81,  73],\n",
              "         [ 92,  90,  79],\n",
              "         [103, 100,  95]],\n",
              "\n",
              "        [[203, 185, 174],\n",
              "         [212, 185, 161],\n",
              "         [127, 112,  85],\n",
              "         ...,\n",
              "         [ 70,  81,  76],\n",
              "         [ 77,  84,  74],\n",
              "         [ 90,  90,  81]],\n",
              "\n",
              "        [[147, 141, 137],\n",
              "         [209, 186, 170],\n",
              "         [145, 130, 111],\n",
              "         ...,\n",
              "         [ 57,  78,  77],\n",
              "         [ 52,  68,  61],\n",
              "         [ 68,  73,  63]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 45,  48,  56],\n",
              "         [ 42,  45,  53],\n",
              "         [ 39,  42,  50],\n",
              "         ...,\n",
              "         [173, 169, 150],\n",
              "         [166, 166, 146],\n",
              "         [168, 171, 149]],\n",
              "\n",
              "        [[ 49,  51,  52],\n",
              "         [ 38,  41,  44],\n",
              "         [ 39,  43,  47],\n",
              "         ...,\n",
              "         [176, 172, 154],\n",
              "         [168, 167, 148],\n",
              "         [166, 168, 148]],\n",
              "\n",
              "        [[128, 129, 120],\n",
              "         [ 80,  81,  78],\n",
              "         [ 50,  53,  52],\n",
              "         ...,\n",
              "         [174, 170, 152],\n",
              "         [165, 164, 148],\n",
              "         [165, 166, 151]]],\n",
              "\n",
              "\n",
              "       ...,\n",
              "\n",
              "\n",
              "       [[[ 72,  85,  89],\n",
              "         [ 70,  84,  85],\n",
              "         [ 81,  95,  96],\n",
              "         ...,\n",
              "         [255, 255, 253],\n",
              "         [255, 255, 253],\n",
              "         [254, 254, 253]],\n",
              "\n",
              "        [[ 68,  73,  71],\n",
              "         [ 71,  80,  76],\n",
              "         [ 76,  89,  88],\n",
              "         ...,\n",
              "         [254, 255, 253],\n",
              "         [253, 255, 253],\n",
              "         [251, 252, 252]],\n",
              "\n",
              "        [[ 66,  71,  67],\n",
              "         [ 72,  79,  73],\n",
              "         [ 71,  81,  78],\n",
              "         ...,\n",
              "         [254, 255, 255],\n",
              "         [243, 248, 248],\n",
              "         [237, 242, 244]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 46,  51,  54],\n",
              "         [ 49,  54,  57],\n",
              "         [ 51,  56,  59],\n",
              "         ...,\n",
              "         [ 82,  75,  66],\n",
              "         [ 85,  81,  75],\n",
              "         [ 88,  87,  86]],\n",
              "\n",
              "        [[ 40,  45,  48],\n",
              "         [ 40,  45,  48],\n",
              "         [ 40,  45,  48],\n",
              "         ...,\n",
              "         [ 77,  70,  61],\n",
              "         [ 78,  73,  67],\n",
              "         [ 79,  78,  77]],\n",
              "\n",
              "        [[ 40,  45,  48],\n",
              "         [ 40,  45,  48],\n",
              "         [ 40,  45,  48],\n",
              "         ...,\n",
              "         [102,  94,  86],\n",
              "         [ 96,  91,  86],\n",
              "         [ 91,  91,  89]]],\n",
              "\n",
              "\n",
              "       [[[ 61,  59,  47],\n",
              "         [ 96,  99,  92],\n",
              "         [111, 116, 111],\n",
              "         ...,\n",
              "         [114, 116, 109],\n",
              "         [114, 116, 109],\n",
              "         [107, 109, 100]],\n",
              "\n",
              "        [[122, 118, 104],\n",
              "         [156, 151, 141],\n",
              "         [153, 148, 139],\n",
              "         ...,\n",
              "         [ 84,  82,  70],\n",
              "         [ 99,  98,  85],\n",
              "         [ 97,  96,  81]],\n",
              "\n",
              "        [[170, 163, 151],\n",
              "         [166, 160, 153],\n",
              "         [132, 122, 112],\n",
              "         ...,\n",
              "         [ 96,  94,  81],\n",
              "         [ 99,  97,  84],\n",
              "         [ 98,  97,  83]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 22,  22,  18],\n",
              "         [  0,   2,   1],\n",
              "         [  5,   5,   2],\n",
              "         ...,\n",
              "         [122, 125, 116],\n",
              "         [134, 136, 128],\n",
              "         [139, 141, 135]],\n",
              "\n",
              "        [[ 30,  29,  26],\n",
              "         [  3,   5,   4],\n",
              "         [  5,   5,   2],\n",
              "         ...,\n",
              "         [141, 144, 135],\n",
              "         [153, 156, 147],\n",
              "         [155, 157, 151]],\n",
              "\n",
              "        [[ 32,  30,  25],\n",
              "         [  5,   7,   5],\n",
              "         [  9,   9,   5],\n",
              "         ...,\n",
              "         [147, 149, 142],\n",
              "         [154, 157, 150],\n",
              "         [154, 157, 152]]],\n",
              "\n",
              "\n",
              "       [[[ 38, 171, 218],\n",
              "         [ 34, 167, 216],\n",
              "         [ 31, 167, 217],\n",
              "         ...,\n",
              "         [  2, 152, 206],\n",
              "         [  2, 152, 207],\n",
              "         [  4, 154, 208]],\n",
              "\n",
              "        [[ 44, 172, 219],\n",
              "         [ 39, 168, 216],\n",
              "         [ 36, 167, 216],\n",
              "         ...,\n",
              "         [  5, 152, 205],\n",
              "         [  5, 152, 206],\n",
              "         [  6, 153, 207]],\n",
              "\n",
              "        [[ 49, 174, 220],\n",
              "         [ 45, 170, 217],\n",
              "         [ 41, 168, 217],\n",
              "         ...,\n",
              "         [  8, 153, 206],\n",
              "         [  8, 153, 206],\n",
              "         [ 10, 154, 207]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[244, 247, 254],\n",
              "         [239, 242, 249],\n",
              "         [237, 240, 247],\n",
              "         ...,\n",
              "         [206, 223, 239],\n",
              "         [207, 224, 240],\n",
              "         [207, 224, 240]],\n",
              "\n",
              "        [[247, 249, 253],\n",
              "         [243, 244, 249],\n",
              "         [242, 243, 247],\n",
              "         ...,\n",
              "         [215, 227, 240],\n",
              "         [214, 227, 240],\n",
              "         [214, 226, 239]],\n",
              "\n",
              "        [[249, 250, 252],\n",
              "         [244, 246, 248],\n",
              "         [245, 246, 248],\n",
              "         ...,\n",
              "         [222, 230, 240],\n",
              "         [221, 229, 239],\n",
              "         [221, 229, 240]]]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3Ku6d157W2d",
        "outputId": "2d44b105-e4a9-4353-dbe4-fbd2f7119e68"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2],\n",
              "       [3],\n",
              "       [1],\n",
              "       ...,\n",
              "       [7],\n",
              "       [8],\n",
              "       [3]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_val.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mk2MZUAn7aMn",
        "outputId": "35bc4a20-b2f0-4a04-a3db-2b205e503bd0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWkXnah57cME",
        "outputId": "8e98c883-c765-47a8-ef3d-f70754bfb19c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(45000, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# One hot encoding the train and validation labels\n",
        "#Y_train = to_categorical(Y_train)\n",
        "#Y_val = to_categorical(Y_val)\n",
        "#y_test = to_categorical(y_test)"
      ],
      "metadata": {
        "id": "kUDjRhuK7jJ4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezCWvWVN8w6L",
        "outputId": "463972b2-416a-43f6-95d6-5f4212c8e953"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Min: %.3f, Max: %.3f' % (X_train.min(), X_train.max()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEkY35ME864O",
        "outputId": "fe7b2590-0f59-44b7-e4a6-d576aff30f29"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Min: 0.000, Max: 255.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ozj0G_eJPiM2",
        "outputId": "a113b72c-d670-4f05-a23d-c2ae1419bf0b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[[163, 177, 156],\n",
              "         [ 91, 116,  85],\n",
              "         [ 92, 126,  87],\n",
              "         ...,\n",
              "         [112, 151, 100],\n",
              "         [ 77, 109,  72],\n",
              "         [134, 153, 130]],\n",
              "\n",
              "        [[100, 124,  86],\n",
              "         [128, 154, 119],\n",
              "         [172, 202, 167],\n",
              "         ...,\n",
              "         [157, 190, 145],\n",
              "         [155, 183, 155],\n",
              "         [ 94, 118,  95]],\n",
              "\n",
              "        [[101, 132,  86],\n",
              "         [154, 184, 142],\n",
              "         [ 83, 118,  68],\n",
              "         ...,\n",
              "         [ 62,  96,  39],\n",
              "         [133, 161, 127],\n",
              "         [139, 167, 136]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[119, 149, 114],\n",
              "         [135, 169, 132],\n",
              "         [ 68, 101,  38],\n",
              "         ...,\n",
              "         [ 52,  89,  18],\n",
              "         [ 94, 132,  76],\n",
              "         [151, 181, 145]],\n",
              "\n",
              "        [[ 90, 117,  83],\n",
              "         [154, 182, 150],\n",
              "         [130, 159, 119],\n",
              "         ...,\n",
              "         [ 90, 124,  77],\n",
              "         [156, 186, 146],\n",
              "         [111, 138, 104]],\n",
              "\n",
              "        [[130, 151, 121],\n",
              "         [ 96, 123,  90],\n",
              "         [156, 185, 152],\n",
              "         ...,\n",
              "         [173, 202, 170],\n",
              "         [113, 141, 107],\n",
              "         [103, 126,  95]]],\n",
              "\n",
              "\n",
              "       [[[173, 165, 153],\n",
              "         [179, 171, 159],\n",
              "         [183, 175, 163],\n",
              "         ...,\n",
              "         [ 63,  57,  57],\n",
              "         [ 15,  14,  20],\n",
              "         [  5,   8,   7]],\n",
              "\n",
              "        [[129, 119, 102],\n",
              "         [142, 132, 115],\n",
              "         [161, 151, 134],\n",
              "         ...,\n",
              "         [ 94,  88,  87],\n",
              "         [ 36,  35,  41],\n",
              "         [  6,   8,   8]],\n",
              "\n",
              "        [[ 98,  85,  62],\n",
              "         [ 93,  80,  57],\n",
              "         [110,  97,  74],\n",
              "         ...,\n",
              "         [ 76,  70,  70],\n",
              "         [ 24,  23,  30],\n",
              "         [  6,   8,   8]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 99,  86,  40],\n",
              "         [ 81,  69,  30],\n",
              "         [102,  85,  50],\n",
              "         ...,\n",
              "         [ 97,  88,  70],\n",
              "         [100,  98,  71],\n",
              "         [122, 139, 112]],\n",
              "\n",
              "        [[139, 128,  76],\n",
              "         [116, 108,  63],\n",
              "         [109,  97,  56],\n",
              "         ...,\n",
              "         [ 73,  76,  60],\n",
              "         [ 97, 112,  91],\n",
              "         [125, 152, 132]],\n",
              "\n",
              "        [[166, 154, 116],\n",
              "         [158, 152, 116],\n",
              "         [151, 142, 111],\n",
              "         ...,\n",
              "         [ 85,  99,  85],\n",
              "         [123, 149, 134],\n",
              "         [143, 178, 164]]],\n",
              "\n",
              "\n",
              "       [[[212, 185, 164],\n",
              "         [200, 176, 150],\n",
              "         [111,  99,  67],\n",
              "         ...,\n",
              "         [ 82,  81,  73],\n",
              "         [ 92,  90,  79],\n",
              "         [103, 100,  95]],\n",
              "\n",
              "        [[203, 185, 174],\n",
              "         [212, 185, 161],\n",
              "         [127, 112,  85],\n",
              "         ...,\n",
              "         [ 70,  81,  76],\n",
              "         [ 77,  84,  74],\n",
              "         [ 90,  90,  81]],\n",
              "\n",
              "        [[147, 141, 137],\n",
              "         [209, 186, 170],\n",
              "         [145, 130, 111],\n",
              "         ...,\n",
              "         [ 57,  78,  77],\n",
              "         [ 52,  68,  61],\n",
              "         [ 68,  73,  63]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 45,  48,  56],\n",
              "         [ 42,  45,  53],\n",
              "         [ 39,  42,  50],\n",
              "         ...,\n",
              "         [173, 169, 150],\n",
              "         [166, 166, 146],\n",
              "         [168, 171, 149]],\n",
              "\n",
              "        [[ 49,  51,  52],\n",
              "         [ 38,  41,  44],\n",
              "         [ 39,  43,  47],\n",
              "         ...,\n",
              "         [176, 172, 154],\n",
              "         [168, 167, 148],\n",
              "         [166, 168, 148]],\n",
              "\n",
              "        [[128, 129, 120],\n",
              "         [ 80,  81,  78],\n",
              "         [ 50,  53,  52],\n",
              "         ...,\n",
              "         [174, 170, 152],\n",
              "         [165, 164, 148],\n",
              "         [165, 166, 151]]],\n",
              "\n",
              "\n",
              "       ...,\n",
              "\n",
              "\n",
              "       [[[ 72,  85,  89],\n",
              "         [ 70,  84,  85],\n",
              "         [ 81,  95,  96],\n",
              "         ...,\n",
              "         [255, 255, 253],\n",
              "         [255, 255, 253],\n",
              "         [254, 254, 253]],\n",
              "\n",
              "        [[ 68,  73,  71],\n",
              "         [ 71,  80,  76],\n",
              "         [ 76,  89,  88],\n",
              "         ...,\n",
              "         [254, 255, 253],\n",
              "         [253, 255, 253],\n",
              "         [251, 252, 252]],\n",
              "\n",
              "        [[ 66,  71,  67],\n",
              "         [ 72,  79,  73],\n",
              "         [ 71,  81,  78],\n",
              "         ...,\n",
              "         [254, 255, 255],\n",
              "         [243, 248, 248],\n",
              "         [237, 242, 244]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 46,  51,  54],\n",
              "         [ 49,  54,  57],\n",
              "         [ 51,  56,  59],\n",
              "         ...,\n",
              "         [ 82,  75,  66],\n",
              "         [ 85,  81,  75],\n",
              "         [ 88,  87,  86]],\n",
              "\n",
              "        [[ 40,  45,  48],\n",
              "         [ 40,  45,  48],\n",
              "         [ 40,  45,  48],\n",
              "         ...,\n",
              "         [ 77,  70,  61],\n",
              "         [ 78,  73,  67],\n",
              "         [ 79,  78,  77]],\n",
              "\n",
              "        [[ 40,  45,  48],\n",
              "         [ 40,  45,  48],\n",
              "         [ 40,  45,  48],\n",
              "         ...,\n",
              "         [102,  94,  86],\n",
              "         [ 96,  91,  86],\n",
              "         [ 91,  91,  89]]],\n",
              "\n",
              "\n",
              "       [[[ 61,  59,  47],\n",
              "         [ 96,  99,  92],\n",
              "         [111, 116, 111],\n",
              "         ...,\n",
              "         [114, 116, 109],\n",
              "         [114, 116, 109],\n",
              "         [107, 109, 100]],\n",
              "\n",
              "        [[122, 118, 104],\n",
              "         [156, 151, 141],\n",
              "         [153, 148, 139],\n",
              "         ...,\n",
              "         [ 84,  82,  70],\n",
              "         [ 99,  98,  85],\n",
              "         [ 97,  96,  81]],\n",
              "\n",
              "        [[170, 163, 151],\n",
              "         [166, 160, 153],\n",
              "         [132, 122, 112],\n",
              "         ...,\n",
              "         [ 96,  94,  81],\n",
              "         [ 99,  97,  84],\n",
              "         [ 98,  97,  83]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 22,  22,  18],\n",
              "         [  0,   2,   1],\n",
              "         [  5,   5,   2],\n",
              "         ...,\n",
              "         [122, 125, 116],\n",
              "         [134, 136, 128],\n",
              "         [139, 141, 135]],\n",
              "\n",
              "        [[ 30,  29,  26],\n",
              "         [  3,   5,   4],\n",
              "         [  5,   5,   2],\n",
              "         ...,\n",
              "         [141, 144, 135],\n",
              "         [153, 156, 147],\n",
              "         [155, 157, 151]],\n",
              "\n",
              "        [[ 32,  30,  25],\n",
              "         [  5,   7,   5],\n",
              "         [  9,   9,   5],\n",
              "         ...,\n",
              "         [147, 149, 142],\n",
              "         [154, 157, 150],\n",
              "         [154, 157, 152]]],\n",
              "\n",
              "\n",
              "       [[[ 38, 171, 218],\n",
              "         [ 34, 167, 216],\n",
              "         [ 31, 167, 217],\n",
              "         ...,\n",
              "         [  2, 152, 206],\n",
              "         [  2, 152, 207],\n",
              "         [  4, 154, 208]],\n",
              "\n",
              "        [[ 44, 172, 219],\n",
              "         [ 39, 168, 216],\n",
              "         [ 36, 167, 216],\n",
              "         ...,\n",
              "         [  5, 152, 205],\n",
              "         [  5, 152, 206],\n",
              "         [  6, 153, 207]],\n",
              "\n",
              "        [[ 49, 174, 220],\n",
              "         [ 45, 170, 217],\n",
              "         [ 41, 168, 217],\n",
              "         ...,\n",
              "         [  8, 153, 206],\n",
              "         [  8, 153, 206],\n",
              "         [ 10, 154, 207]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[244, 247, 254],\n",
              "         [239, 242, 249],\n",
              "         [237, 240, 247],\n",
              "         ...,\n",
              "         [206, 223, 239],\n",
              "         [207, 224, 240],\n",
              "         [207, 224, 240]],\n",
              "\n",
              "        [[247, 249, 253],\n",
              "         [243, 244, 249],\n",
              "         [242, 243, 247],\n",
              "         ...,\n",
              "         [215, 227, 240],\n",
              "         [214, 227, 240],\n",
              "         [214, 226, 239]],\n",
              "\n",
              "        [[249, 250, 252],\n",
              "         [244, 246, 248],\n",
              "         [245, 246, 248],\n",
              "         ...,\n",
              "         [222, 230, 240],\n",
              "         [221, 229, 239],\n",
              "         [221, 229, 240]]]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train = X_train.astype('float32')\n",
        "#X_val = X_val.astype('float32')\n",
        "#x_test = x_test.astype('float32')\n",
        "\n",
        "X_train = X_train/255\n",
        "X_val = X_val/255\n",
        "x_test = x_test/255"
      ],
      "metadata": {
        "id": "1LiG8WHf4iN6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Min: %.3f, Max: %.3f' % (X_train.min(), X_train.max()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snOeVpgA5OiJ",
        "outputId": "60522b69-2122-474b-97b3-bac5c2259f53"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Min: 0.000, Max: 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Min: %.3f, Max: %.3f' % (X_val.min(), X_val.max()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezgAsW4T_Z98",
        "outputId": "736842ac-ac41-4daa-fdde-d70f9fc36550"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Min: 0.000, Max: 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Min: %.3f, Max: %.3f' % (x_test.min(), x_test.max()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adGrZibv_dAM",
        "outputId": "f48ec88d-87a4-4626-a7e1-736b5f0e0808"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Min: 0.000, Max: 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rescale to [-1,1]"
      ],
      "metadata": {
        "id": "EbAZxGIQ_pHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rescale(data):\n",
        "  data = tf.math.divide(\n",
        "   tf.subtract(\n",
        "      data, \n",
        "      tf.reduce_min(data)\n",
        "   ), \n",
        "   tf.subtract(\n",
        "      tf.reduce_max(data), \n",
        "      tf.reduce_min(data)\n",
        "   )\n",
        "  )\n",
        "  return 2*data-1"
      ],
      "metadata": {
        "id": "Vg-PYDdpjuWX"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_new = tf.convert_to_tensor(X_train)\n",
        "X_val_new = tf.convert_to_tensor(X_val)\n",
        "x_test_new = tf.convert_to_tensor(x_test)"
      ],
      "metadata": {
        "id": "sUNaznUkqSsN"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_new = rescale(X_train_new)\n",
        "X_val_new = rescale(X_val_new)\n",
        "x_test_new = rescale(x_test_new)"
      ],
      "metadata": {
        "id": "5cqBe8rJ_gCf"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test_new.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAseGKVaAoV_",
        "outputId": "12245f04-3e78-4f84-e38f-b231bf5188b2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([10000, 32, 32, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(x_test_new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpFoljDcBkIR",
        "outputId": "ee06c19f-d6b5-4c87-acff-f39fe479b587"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.framework.ops.EagerTensor"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Architecture:\n",
        "#### 1st 2d conv layer: there are 10 kernels whose size is 5x5x3; stride=1\n",
        "#### Maxpooling: 2x2 with stride=2\n",
        "#### 1st 2d conv layer: there are 10 kernels whose size is 5x5x10; stride=1\n",
        "#### Maxpooling: 2x2 with stride=2\n",
        "#### 1st fully-connected layer: [flattened final feature map] x 20\n",
        "#### 2st fully-connected layer: 20 x 10 Softmax on the 10 classes"
      ],
      "metadata": {
        "id": "D436MJMrDFJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kaiming_he = keras.initializers.HeNormal()"
      ],
      "metadata": {
        "id": "8p1MRPTyJPVj"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1 = keras.Sequential([\n",
        "              layers.Conv2D(filters=10, kernel_size=(5,5), strides=(1,1), activation='relu', kernel_initializer=kaiming_he, data_format = 'channels_last', input_shape=(32,32,3)),\n",
        "              layers.MaxPooling2D(pool_size = (2,2), strides = (2,2)),\n",
        "              layers.Conv2D(filters=10, kernel_size=(5,5), strides=(1,1), activation='relu',kernel_initializer=kaiming_he, data_format = 'channels_last'),\n",
        "              layers.MaxPooling2D(pool_size = (2,2), strides = (2,2)),\n",
        "              layers.Flatten(),\n",
        "              layers.Dense(units=20, activation=\"relu\", kernel_initializer=kaiming_he),\n",
        "              layers.Dense(units=10, activation=\"softmax\", kernel_initializer=kaiming_he)\n",
        "])"
      ],
      "metadata": {
        "id": "tXxcmginCR6j"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=[\"acc\"])"
      ],
      "metadata": {
        "id": "oxFrL7WsMp0Z"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_1 = model_1.fit(X_train_new, Y_train, epochs=200, batch_size=64, validation_data=(X_val_new, Y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRLUmthmNcWa",
        "outputId": "fd7d4b02-7017-43ed-dda1-0ab0f0118f74"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "704/704 [==============================] - 5s 5ms/step - loss: 1.7638 - acc: 0.3601 - val_loss: 1.5100 - val_acc: 0.4666\n",
            "Epoch 2/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 1.4019 - acc: 0.4978 - val_loss: 1.3380 - val_acc: 0.5296\n",
            "Epoch 3/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 1.2953 - acc: 0.5380 - val_loss: 1.2789 - val_acc: 0.5480\n",
            "Epoch 4/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 1.2323 - acc: 0.5622 - val_loss: 1.2242 - val_acc: 0.5718\n",
            "Epoch 5/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 1.1861 - acc: 0.5787 - val_loss: 1.2271 - val_acc: 0.5742\n",
            "Epoch 6/200\n",
            "704/704 [==============================] - 4s 5ms/step - loss: 1.1572 - acc: 0.5914 - val_loss: 1.1487 - val_acc: 0.5972\n",
            "Epoch 7/200\n",
            "704/704 [==============================] - 4s 5ms/step - loss: 1.1305 - acc: 0.6007 - val_loss: 1.1779 - val_acc: 0.5948\n",
            "Epoch 8/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 1.1068 - acc: 0.6119 - val_loss: 1.1338 - val_acc: 0.6072\n",
            "Epoch 9/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 1.0849 - acc: 0.6190 - val_loss: 1.1205 - val_acc: 0.6162\n",
            "Epoch 10/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 1.0718 - acc: 0.6223 - val_loss: 1.1182 - val_acc: 0.6204\n",
            "Epoch 11/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 1.0564 - acc: 0.6279 - val_loss: 1.1047 - val_acc: 0.6244\n",
            "Epoch 12/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 1.0409 - acc: 0.6342 - val_loss: 1.1143 - val_acc: 0.6180\n",
            "Epoch 13/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 1.0289 - acc: 0.6393 - val_loss: 1.0567 - val_acc: 0.6430\n",
            "Epoch 14/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 1.0181 - acc: 0.6421 - val_loss: 1.0652 - val_acc: 0.6352\n",
            "Epoch 15/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 1.0125 - acc: 0.6457 - val_loss: 1.0596 - val_acc: 0.6406\n",
            "Epoch 16/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 1.0022 - acc: 0.6489 - val_loss: 1.0582 - val_acc: 0.6412\n",
            "Epoch 17/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.9940 - acc: 0.6498 - val_loss: 1.0442 - val_acc: 0.6450\n",
            "Epoch 18/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.9883 - acc: 0.6545 - val_loss: 1.0501 - val_acc: 0.6426\n",
            "Epoch 19/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.9793 - acc: 0.6579 - val_loss: 1.0288 - val_acc: 0.6488\n",
            "Epoch 20/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.9730 - acc: 0.6588 - val_loss: 1.0610 - val_acc: 0.6492\n",
            "Epoch 21/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.9646 - acc: 0.6648 - val_loss: 1.0654 - val_acc: 0.6354\n",
            "Epoch 22/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.9622 - acc: 0.6619 - val_loss: 1.0518 - val_acc: 0.6458\n",
            "Epoch 23/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.9510 - acc: 0.6669 - val_loss: 1.0163 - val_acc: 0.6542\n",
            "Epoch 24/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.9452 - acc: 0.6697 - val_loss: 1.0207 - val_acc: 0.6516\n",
            "Epoch 25/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.9399 - acc: 0.6728 - val_loss: 1.0201 - val_acc: 0.6510\n",
            "Epoch 26/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.9363 - acc: 0.6742 - val_loss: 1.0201 - val_acc: 0.6590\n",
            "Epoch 27/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.9279 - acc: 0.6765 - val_loss: 1.0051 - val_acc: 0.6592\n",
            "Epoch 28/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.9251 - acc: 0.6770 - val_loss: 1.0448 - val_acc: 0.6468\n",
            "Epoch 29/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.9207 - acc: 0.6784 - val_loss: 1.0085 - val_acc: 0.6542\n",
            "Epoch 30/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.9159 - acc: 0.6799 - val_loss: 0.9987 - val_acc: 0.6536\n",
            "Epoch 31/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.9133 - acc: 0.6806 - val_loss: 1.0346 - val_acc: 0.6528\n",
            "Epoch 32/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.9082 - acc: 0.6826 - val_loss: 1.0486 - val_acc: 0.6488\n",
            "Epoch 33/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.9103 - acc: 0.6820 - val_loss: 1.0035 - val_acc: 0.6608\n",
            "Epoch 34/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8994 - acc: 0.6848 - val_loss: 0.9921 - val_acc: 0.6642\n",
            "Epoch 35/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.9022 - acc: 0.6824 - val_loss: 1.0380 - val_acc: 0.6540\n",
            "Epoch 36/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8952 - acc: 0.6870 - val_loss: 1.0027 - val_acc: 0.6612\n",
            "Epoch 37/200\n",
            "704/704 [==============================] - 4s 5ms/step - loss: 0.8935 - acc: 0.6861 - val_loss: 1.0312 - val_acc: 0.6516\n",
            "Epoch 38/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8915 - acc: 0.6871 - val_loss: 1.0275 - val_acc: 0.6560\n",
            "Epoch 39/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8860 - acc: 0.6892 - val_loss: 1.0135 - val_acc: 0.6606\n",
            "Epoch 40/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8864 - acc: 0.6909 - val_loss: 1.0095 - val_acc: 0.6552\n",
            "Epoch 41/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8834 - acc: 0.6899 - val_loss: 0.9980 - val_acc: 0.6598\n",
            "Epoch 42/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8822 - acc: 0.6903 - val_loss: 1.0109 - val_acc: 0.6552\n",
            "Epoch 43/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8814 - acc: 0.6919 - val_loss: 1.0836 - val_acc: 0.6374\n",
            "Epoch 44/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8748 - acc: 0.6921 - val_loss: 1.0168 - val_acc: 0.6562\n",
            "Epoch 45/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8750 - acc: 0.6925 - val_loss: 1.0468 - val_acc: 0.6548\n",
            "Epoch 46/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8752 - acc: 0.6907 - val_loss: 0.9975 - val_acc: 0.6636\n",
            "Epoch 47/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8756 - acc: 0.6910 - val_loss: 1.0082 - val_acc: 0.6674\n",
            "Epoch 48/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8677 - acc: 0.6964 - val_loss: 1.0352 - val_acc: 0.6482\n",
            "Epoch 49/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8670 - acc: 0.6957 - val_loss: 0.9959 - val_acc: 0.6682\n",
            "Epoch 50/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8615 - acc: 0.6961 - val_loss: 1.0074 - val_acc: 0.6640\n",
            "Epoch 51/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8646 - acc: 0.6942 - val_loss: 0.9980 - val_acc: 0.6682\n",
            "Epoch 52/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8589 - acc: 0.6981 - val_loss: 1.0054 - val_acc: 0.6720\n",
            "Epoch 53/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8623 - acc: 0.6964 - val_loss: 1.0172 - val_acc: 0.6586\n",
            "Epoch 54/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8541 - acc: 0.7008 - val_loss: 1.0156 - val_acc: 0.6626\n",
            "Epoch 55/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8557 - acc: 0.6988 - val_loss: 1.0210 - val_acc: 0.6566\n",
            "Epoch 56/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8533 - acc: 0.7000 - val_loss: 0.9889 - val_acc: 0.6668\n",
            "Epoch 57/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8535 - acc: 0.6999 - val_loss: 0.9969 - val_acc: 0.6610\n",
            "Epoch 58/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8491 - acc: 0.7021 - val_loss: 1.0252 - val_acc: 0.6640\n",
            "Epoch 59/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8477 - acc: 0.6999 - val_loss: 1.0329 - val_acc: 0.6596\n",
            "Epoch 60/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8482 - acc: 0.7005 - val_loss: 0.9869 - val_acc: 0.6718\n",
            "Epoch 61/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8487 - acc: 0.7002 - val_loss: 0.9966 - val_acc: 0.6602\n",
            "Epoch 62/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8457 - acc: 0.7010 - val_loss: 1.0036 - val_acc: 0.6636\n",
            "Epoch 63/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8415 - acc: 0.7042 - val_loss: 1.0258 - val_acc: 0.6594\n",
            "Epoch 64/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8430 - acc: 0.7033 - val_loss: 0.9963 - val_acc: 0.6666\n",
            "Epoch 65/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8451 - acc: 0.7019 - val_loss: 1.0076 - val_acc: 0.6604\n",
            "Epoch 66/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8386 - acc: 0.7026 - val_loss: 1.0319 - val_acc: 0.6510\n",
            "Epoch 67/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8404 - acc: 0.7035 - val_loss: 1.0066 - val_acc: 0.6644\n",
            "Epoch 68/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8344 - acc: 0.7056 - val_loss: 1.0114 - val_acc: 0.6634\n",
            "Epoch 69/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8360 - acc: 0.7049 - val_loss: 1.0218 - val_acc: 0.6626\n",
            "Epoch 70/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8352 - acc: 0.7057 - val_loss: 1.0084 - val_acc: 0.6632\n",
            "Epoch 71/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8368 - acc: 0.7063 - val_loss: 1.0317 - val_acc: 0.6600\n",
            "Epoch 72/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8316 - acc: 0.7071 - val_loss: 1.0325 - val_acc: 0.6596\n",
            "Epoch 73/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8298 - acc: 0.7077 - val_loss: 1.0134 - val_acc: 0.6602\n",
            "Epoch 74/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8307 - acc: 0.7067 - val_loss: 1.0314 - val_acc: 0.6580\n",
            "Epoch 75/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8286 - acc: 0.7053 - val_loss: 1.0302 - val_acc: 0.6546\n",
            "Epoch 76/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8306 - acc: 0.7072 - val_loss: 1.0196 - val_acc: 0.6622\n",
            "Epoch 77/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8251 - acc: 0.7098 - val_loss: 1.0173 - val_acc: 0.6622\n",
            "Epoch 78/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8232 - acc: 0.7092 - val_loss: 1.0185 - val_acc: 0.6648\n",
            "Epoch 79/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8211 - acc: 0.7106 - val_loss: 1.0215 - val_acc: 0.6540\n",
            "Epoch 80/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8223 - acc: 0.7100 - val_loss: 1.0227 - val_acc: 0.6646\n",
            "Epoch 81/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8237 - acc: 0.7099 - val_loss: 1.0198 - val_acc: 0.6598\n",
            "Epoch 82/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8259 - acc: 0.7071 - val_loss: 1.0126 - val_acc: 0.6564\n",
            "Epoch 83/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8231 - acc: 0.7088 - val_loss: 1.0336 - val_acc: 0.6542\n",
            "Epoch 84/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8168 - acc: 0.7099 - val_loss: 1.0179 - val_acc: 0.6636\n",
            "Epoch 85/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8162 - acc: 0.7125 - val_loss: 1.0042 - val_acc: 0.6652\n",
            "Epoch 86/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8177 - acc: 0.7126 - val_loss: 1.0083 - val_acc: 0.6636\n",
            "Epoch 87/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8165 - acc: 0.7115 - val_loss: 1.0192 - val_acc: 0.6672\n",
            "Epoch 88/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8147 - acc: 0.7110 - val_loss: 1.0414 - val_acc: 0.6494\n",
            "Epoch 89/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8164 - acc: 0.7106 - val_loss: 1.0243 - val_acc: 0.6608\n",
            "Epoch 90/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8121 - acc: 0.7142 - val_loss: 1.0545 - val_acc: 0.6516\n",
            "Epoch 91/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8111 - acc: 0.7138 - val_loss: 1.0183 - val_acc: 0.6606\n",
            "Epoch 92/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8086 - acc: 0.7138 - val_loss: 1.0106 - val_acc: 0.6666\n",
            "Epoch 93/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8095 - acc: 0.7132 - val_loss: 1.0173 - val_acc: 0.6600\n",
            "Epoch 94/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8087 - acc: 0.7138 - val_loss: 1.0299 - val_acc: 0.6556\n",
            "Epoch 95/200\n",
            "704/704 [==============================] - 4s 5ms/step - loss: 0.8096 - acc: 0.7132 - val_loss: 1.0432 - val_acc: 0.6520\n",
            "Epoch 96/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8096 - acc: 0.7148 - val_loss: 1.0258 - val_acc: 0.6638\n",
            "Epoch 97/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8082 - acc: 0.7130 - val_loss: 1.0237 - val_acc: 0.6586\n",
            "Epoch 98/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8109 - acc: 0.7120 - val_loss: 1.0232 - val_acc: 0.6586\n",
            "Epoch 99/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8028 - acc: 0.7160 - val_loss: 1.0500 - val_acc: 0.6492\n",
            "Epoch 100/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8034 - acc: 0.7146 - val_loss: 1.0033 - val_acc: 0.6616\n",
            "Epoch 101/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8039 - acc: 0.7164 - val_loss: 1.0174 - val_acc: 0.6582\n",
            "Epoch 102/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8066 - acc: 0.7148 - val_loss: 1.0364 - val_acc: 0.6584\n",
            "Epoch 103/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8024 - acc: 0.7173 - val_loss: 1.0277 - val_acc: 0.6562\n",
            "Epoch 104/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8024 - acc: 0.7144 - val_loss: 1.0153 - val_acc: 0.6636\n",
            "Epoch 105/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8032 - acc: 0.7157 - val_loss: 1.0273 - val_acc: 0.6558\n",
            "Epoch 106/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8008 - acc: 0.7170 - val_loss: 1.0635 - val_acc: 0.6470\n",
            "Epoch 107/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7990 - acc: 0.7178 - val_loss: 1.0309 - val_acc: 0.6614\n",
            "Epoch 108/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7994 - acc: 0.7175 - val_loss: 1.0300 - val_acc: 0.6568\n",
            "Epoch 109/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.8007 - acc: 0.7159 - val_loss: 1.0397 - val_acc: 0.6544\n",
            "Epoch 110/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7982 - acc: 0.7162 - val_loss: 1.0256 - val_acc: 0.6590\n",
            "Epoch 111/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7957 - acc: 0.7200 - val_loss: 1.0408 - val_acc: 0.6540\n",
            "Epoch 112/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7978 - acc: 0.7168 - val_loss: 1.0355 - val_acc: 0.6622\n",
            "Epoch 113/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7970 - acc: 0.7181 - val_loss: 1.0400 - val_acc: 0.6554\n",
            "Epoch 114/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7984 - acc: 0.7192 - val_loss: 1.0106 - val_acc: 0.6692\n",
            "Epoch 115/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7944 - acc: 0.7191 - val_loss: 1.0890 - val_acc: 0.6354\n",
            "Epoch 116/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7971 - acc: 0.7176 - val_loss: 1.0138 - val_acc: 0.6618\n",
            "Epoch 117/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7939 - acc: 0.7194 - val_loss: 1.0265 - val_acc: 0.6612\n",
            "Epoch 118/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7942 - acc: 0.7177 - val_loss: 1.0400 - val_acc: 0.6510\n",
            "Epoch 119/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7943 - acc: 0.7206 - val_loss: 1.0260 - val_acc: 0.6546\n",
            "Epoch 120/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7953 - acc: 0.7192 - val_loss: 1.0480 - val_acc: 0.6514\n",
            "Epoch 121/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7934 - acc: 0.7196 - val_loss: 1.0319 - val_acc: 0.6552\n",
            "Epoch 122/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7907 - acc: 0.7207 - val_loss: 1.0425 - val_acc: 0.6580\n",
            "Epoch 123/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7930 - acc: 0.7181 - val_loss: 1.0598 - val_acc: 0.6542\n",
            "Epoch 124/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7914 - acc: 0.7204 - val_loss: 1.0360 - val_acc: 0.6528\n",
            "Epoch 125/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7898 - acc: 0.7186 - val_loss: 1.0545 - val_acc: 0.6520\n",
            "Epoch 126/200\n",
            "704/704 [==============================] - 3s 5ms/step - loss: 0.7894 - acc: 0.7202 - val_loss: 1.0390 - val_acc: 0.6568\n",
            "Epoch 127/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7898 - acc: 0.7207 - val_loss: 1.0728 - val_acc: 0.6456\n",
            "Epoch 128/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7863 - acc: 0.7195 - val_loss: 1.0302 - val_acc: 0.6612\n",
            "Epoch 129/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7855 - acc: 0.7244 - val_loss: 1.0434 - val_acc: 0.6560\n",
            "Epoch 130/200\n",
            "704/704 [==============================] - 3s 5ms/step - loss: 0.7872 - acc: 0.7203 - val_loss: 1.0543 - val_acc: 0.6592\n",
            "Epoch 131/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7885 - acc: 0.7202 - val_loss: 1.0191 - val_acc: 0.6672\n",
            "Epoch 132/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7860 - acc: 0.7197 - val_loss: 1.0399 - val_acc: 0.6580\n",
            "Epoch 133/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7846 - acc: 0.7218 - val_loss: 1.0337 - val_acc: 0.6576\n",
            "Epoch 134/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7853 - acc: 0.7214 - val_loss: 1.0128 - val_acc: 0.6652\n",
            "Epoch 135/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7886 - acc: 0.7195 - val_loss: 1.0683 - val_acc: 0.6500\n",
            "Epoch 136/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7850 - acc: 0.7213 - val_loss: 1.0562 - val_acc: 0.6470\n",
            "Epoch 137/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7870 - acc: 0.7203 - val_loss: 1.0342 - val_acc: 0.6582\n",
            "Epoch 138/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7838 - acc: 0.7202 - val_loss: 1.0305 - val_acc: 0.6558\n",
            "Epoch 139/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7812 - acc: 0.7237 - val_loss: 1.0612 - val_acc: 0.6462\n",
            "Epoch 140/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7836 - acc: 0.7219 - val_loss: 1.0421 - val_acc: 0.6560\n",
            "Epoch 141/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7818 - acc: 0.7239 - val_loss: 1.0517 - val_acc: 0.6570\n",
            "Epoch 142/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7810 - acc: 0.7231 - val_loss: 1.0393 - val_acc: 0.6582\n",
            "Epoch 143/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7814 - acc: 0.7224 - val_loss: 1.0408 - val_acc: 0.6596\n",
            "Epoch 144/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7812 - acc: 0.7222 - val_loss: 1.0320 - val_acc: 0.6552\n",
            "Epoch 145/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7827 - acc: 0.7208 - val_loss: 1.0521 - val_acc: 0.6514\n",
            "Epoch 146/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7782 - acc: 0.7236 - val_loss: 1.0441 - val_acc: 0.6584\n",
            "Epoch 147/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7779 - acc: 0.7250 - val_loss: 1.0458 - val_acc: 0.6554\n",
            "Epoch 148/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7773 - acc: 0.7238 - val_loss: 1.0423 - val_acc: 0.6562\n",
            "Epoch 149/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7786 - acc: 0.7240 - val_loss: 1.0348 - val_acc: 0.6600\n",
            "Epoch 150/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7795 - acc: 0.7220 - val_loss: 1.0453 - val_acc: 0.6558\n",
            "Epoch 151/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7785 - acc: 0.7233 - val_loss: 1.0353 - val_acc: 0.6566\n",
            "Epoch 152/200\n",
            "704/704 [==============================] - 4s 5ms/step - loss: 0.7754 - acc: 0.7255 - val_loss: 1.0453 - val_acc: 0.6554\n",
            "Epoch 153/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7778 - acc: 0.7245 - val_loss: 1.0433 - val_acc: 0.6624\n",
            "Epoch 154/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7749 - acc: 0.7262 - val_loss: 1.0414 - val_acc: 0.6604\n",
            "Epoch 155/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7734 - acc: 0.7242 - val_loss: 1.0528 - val_acc: 0.6574\n",
            "Epoch 156/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7774 - acc: 0.7239 - val_loss: 1.0375 - val_acc: 0.6576\n",
            "Epoch 157/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7763 - acc: 0.7240 - val_loss: 1.0888 - val_acc: 0.6420\n",
            "Epoch 158/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7748 - acc: 0.7258 - val_loss: 1.0392 - val_acc: 0.6564\n",
            "Epoch 159/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7741 - acc: 0.7255 - val_loss: 1.0582 - val_acc: 0.6590\n",
            "Epoch 160/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7747 - acc: 0.7246 - val_loss: 1.0469 - val_acc: 0.6530\n",
            "Epoch 161/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7721 - acc: 0.7250 - val_loss: 1.0774 - val_acc: 0.6410\n",
            "Epoch 162/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7743 - acc: 0.7258 - val_loss: 1.0514 - val_acc: 0.6542\n",
            "Epoch 163/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7710 - acc: 0.7263 - val_loss: 1.0525 - val_acc: 0.6616\n",
            "Epoch 164/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7698 - acc: 0.7275 - val_loss: 1.0671 - val_acc: 0.6504\n",
            "Epoch 165/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7729 - acc: 0.7251 - val_loss: 1.0620 - val_acc: 0.6566\n",
            "Epoch 166/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7754 - acc: 0.7264 - val_loss: 1.0775 - val_acc: 0.6540\n",
            "Epoch 167/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7714 - acc: 0.7265 - val_loss: 1.0753 - val_acc: 0.6498\n",
            "Epoch 168/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7716 - acc: 0.7264 - val_loss: 1.0406 - val_acc: 0.6554\n",
            "Epoch 169/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7681 - acc: 0.7256 - val_loss: 1.0527 - val_acc: 0.6588\n",
            "Epoch 170/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7705 - acc: 0.7252 - val_loss: 1.0495 - val_acc: 0.6676\n",
            "Epoch 171/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7696 - acc: 0.7258 - val_loss: 1.0507 - val_acc: 0.6572\n",
            "Epoch 172/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7692 - acc: 0.7283 - val_loss: 1.0533 - val_acc: 0.6544\n",
            "Epoch 173/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7673 - acc: 0.7277 - val_loss: 1.0785 - val_acc: 0.6490\n",
            "Epoch 174/200\n",
            "704/704 [==============================] - 3s 5ms/step - loss: 0.7675 - acc: 0.7275 - val_loss: 1.0567 - val_acc: 0.6544\n",
            "Epoch 175/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7653 - acc: 0.7289 - val_loss: 1.0727 - val_acc: 0.6508\n",
            "Epoch 176/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7670 - acc: 0.7282 - val_loss: 1.0509 - val_acc: 0.6568\n",
            "Epoch 177/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7692 - acc: 0.7277 - val_loss: 1.0501 - val_acc: 0.6604\n",
            "Epoch 178/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7641 - acc: 0.7287 - val_loss: 1.0559 - val_acc: 0.6560\n",
            "Epoch 179/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7684 - acc: 0.7268 - val_loss: 1.0556 - val_acc: 0.6530\n",
            "Epoch 180/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7659 - acc: 0.7265 - val_loss: 1.0589 - val_acc: 0.6496\n",
            "Epoch 181/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7665 - acc: 0.7268 - val_loss: 1.0552 - val_acc: 0.6576\n",
            "Epoch 182/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7668 - acc: 0.7280 - val_loss: 1.0807 - val_acc: 0.6508\n",
            "Epoch 183/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7659 - acc: 0.7281 - val_loss: 1.0457 - val_acc: 0.6616\n",
            "Epoch 184/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7650 - acc: 0.7300 - val_loss: 1.0894 - val_acc: 0.6442\n",
            "Epoch 185/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7657 - acc: 0.7294 - val_loss: 1.0671 - val_acc: 0.6556\n",
            "Epoch 186/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7658 - acc: 0.7280 - val_loss: 1.0625 - val_acc: 0.6536\n",
            "Epoch 187/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7644 - acc: 0.7272 - val_loss: 1.0463 - val_acc: 0.6592\n",
            "Epoch 188/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7645 - acc: 0.7297 - val_loss: 1.0382 - val_acc: 0.6606\n",
            "Epoch 189/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7615 - acc: 0.7308 - val_loss: 1.0646 - val_acc: 0.6592\n",
            "Epoch 190/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7643 - acc: 0.7271 - val_loss: 1.0918 - val_acc: 0.6466\n",
            "Epoch 191/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7631 - acc: 0.7302 - val_loss: 1.0440 - val_acc: 0.6612\n",
            "Epoch 192/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7608 - acc: 0.7312 - val_loss: 1.0616 - val_acc: 0.6608\n",
            "Epoch 193/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7626 - acc: 0.7292 - val_loss: 1.0603 - val_acc: 0.6586\n",
            "Epoch 194/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7632 - acc: 0.7291 - val_loss: 1.0564 - val_acc: 0.6516\n",
            "Epoch 195/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7639 - acc: 0.7306 - val_loss: 1.0678 - val_acc: 0.6550\n",
            "Epoch 196/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7617 - acc: 0.7304 - val_loss: 1.0900 - val_acc: 0.6462\n",
            "Epoch 197/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7630 - acc: 0.7289 - val_loss: 1.0602 - val_acc: 0.6500\n",
            "Epoch 198/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7630 - acc: 0.7288 - val_loss: 1.0474 - val_acc: 0.6624\n",
            "Epoch 199/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7632 - acc: 0.7283 - val_loss: 1.0550 - val_acc: 0.6626\n",
            "Epoch 200/200\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 0.7629 - acc: 0.7290 - val_loss: 1.0720 - val_acc: 0.6554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-Oy4ux8vsmx",
        "outputId": "b0b66ee0-d62e-4f14-a745-dee62c90935d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/gdrive/MyDrive/DLS_Assignments/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPaPHkThv7H9",
        "outputId": "0675fb2b-134a-4124-f0ad-f648c81abdc1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Assignment2 Data'   Models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/gdrive/MyDrive/DLS_Assignments/Models/assign3_model_1.h5'\n",
        "model_1.save(path)"
      ],
      "metadata": {
        "id": "KM78SjH_tlwe"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model(path)"
      ],
      "metadata": {
        "id": "v2tB5RzYzMXB"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "byqvOUPQzTLx"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graph for Model_1"
      ],
      "metadata": {
        "id": "sNLaBqKSWxDj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation"
      ],
      "metadata": {
        "id": "dKozQAO4Wnf1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data 1"
      ],
      "metadata": {
        "id": "sConb6eMZb3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_1 = np.minimum(1.1*X_train, 1)\n",
        "X_train_1 = tf.convert_to_tensor(X_train_1)\n",
        "X_train_1 = tf.cast(X_train_1, tf.float32)\n",
        "X_train_1 = rescale(X_train_1) "
      ],
      "metadata": {
        "id": "2JDDK5IgOdNm"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data 2"
      ],
      "metadata": {
        "id": "HpXFIzp9Zk2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_2 = 0.9*X_train\n",
        "X_train_2 = tf.convert_to_tensor(X_train_2)\n",
        "X_train_2 = tf.cast(X_train_2, tf.float32) \n",
        "X_train_2 = rescale(X_train_2) "
      ],
      "metadata": {
        "id": "vh371XFJZYmo"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data 3"
      ],
      "metadata": {
        "id": "zb1zctZXZviv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_3 = tf.image.flip_left_right(X_train)\n",
        "X_train_3 = tf.convert_to_tensor(X_train_3)\n",
        "X_train_3 = tf.cast(X_train_3, tf.float32)\n",
        "X_train_3 = rescale(X_train_3)  "
      ],
      "metadata": {
        "id": "gtwzx74DZx8h"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data 4"
      ],
      "metadata": {
        "id": "JG3wCAmaaQEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_4 = X_train\n",
        "X_train_4 = tf.convert_to_tensor(X_train_4)\n",
        "X_train_4 = tf.cast(X_train_4, tf.float32)\n",
        "X_train_4 = rescale(X_train_4)  "
      ],
      "metadata": {
        "id": "-tUNiwdqaR5G"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val_aug = tf.convert_to_tensor(X_val)\n",
        "X_val_aug = rescale(X_val_aug)"
      ],
      "metadata": {
        "id": "FX4LPGsHaqAL"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Augmented Dataset"
      ],
      "metadata": {
        "id": "fghDN3y4aqbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Aug_X_train = tf.concat([X_train_1,X_train_2,X_train_3,X_train_4],0)"
      ],
      "metadata": {
        "id": "wpPsDUpCatDJ"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train_aug = tf.concat([Y_train,Y_train,Y_train,Y_train],0)"
      ],
      "metadata": {
        "id": "LKrcCTu3syyw"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retraining the Model on the Augmented Dataset"
      ],
      "metadata": {
        "id": "bi0pcwGyZQ1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_2 = keras.Sequential([\n",
        "              layers.Conv2D(filters=10, kernel_size=(5,5), strides=(1,1), activation='relu', kernel_initializer=kaiming_he, data_format = 'channels_last', input_shape=(32,32,3)),\n",
        "              layers.MaxPooling2D(pool_size = (2,2), strides = (2,2)),\n",
        "              layers.Conv2D(filters=10, kernel_size=(5,5), strides=(1,1), activation='relu',kernel_initializer=kaiming_he, data_format = 'channels_last'),\n",
        "              layers.MaxPooling2D(pool_size = (2,2), strides = (2,2)),\n",
        "              layers.Flatten(),\n",
        "              layers.Dense(units=20, activation=\"relu\", kernel_initializer=kaiming_he),\n",
        "              layers.Dense(units=10, activation=\"softmax\", kernel_initializer=kaiming_he)\n",
        "])"
      ],
      "metadata": {
        "id": "-DX6wj3nY1UQ"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=[\"acc\"])"
      ],
      "metadata": {
        "id": "aK63pa2LZjAZ"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_2 = model_2.fit(Aug_X_train, Y_train_aug, epochs=200, batch_size=512, validation_data=(X_val_aug, Y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7CfPq2OZnOf",
        "outputId": "c3cc7505-faca-4e5a-e375-15150f151259"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "352/352 [==============================] - 5s 10ms/step - loss: 1.6937 - acc: 0.3772 - val_loss: 1.4619 - val_acc: 0.4854\n",
            "Epoch 2/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 1.3914 - acc: 0.5014 - val_loss: 1.3312 - val_acc: 0.5388\n",
            "Epoch 3/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 1.3022 - acc: 0.5363 - val_loss: 1.2702 - val_acc: 0.5558\n",
            "Epoch 4/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 1.2196 - acc: 0.5681 - val_loss: 1.1880 - val_acc: 0.5890\n",
            "Epoch 5/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 1.1639 - acc: 0.5912 - val_loss: 1.1617 - val_acc: 0.6008\n",
            "Epoch 6/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 1.1249 - acc: 0.6059 - val_loss: 1.1207 - val_acc: 0.6182\n",
            "Epoch 7/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 1.0989 - acc: 0.6161 - val_loss: 1.0971 - val_acc: 0.6202\n",
            "Epoch 8/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 1.0740 - acc: 0.6268 - val_loss: 1.0736 - val_acc: 0.6338\n",
            "Epoch 9/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 1.0536 - acc: 0.6335 - val_loss: 1.0702 - val_acc: 0.6356\n",
            "Epoch 10/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 1.0385 - acc: 0.6393 - val_loss: 1.0720 - val_acc: 0.6322\n",
            "Epoch 11/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 1.0241 - acc: 0.6444 - val_loss: 1.0473 - val_acc: 0.6466\n",
            "Epoch 12/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 1.0079 - acc: 0.6504 - val_loss: 1.0252 - val_acc: 0.6488\n",
            "Epoch 13/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.9974 - acc: 0.6549 - val_loss: 1.0352 - val_acc: 0.6466\n",
            "Epoch 14/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.9845 - acc: 0.6595 - val_loss: 1.0413 - val_acc: 0.6466\n",
            "Epoch 15/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.9758 - acc: 0.6632 - val_loss: 1.0145 - val_acc: 0.6530\n",
            "Epoch 16/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.9678 - acc: 0.6654 - val_loss: 0.9995 - val_acc: 0.6578\n",
            "Epoch 17/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.9594 - acc: 0.6678 - val_loss: 0.9902 - val_acc: 0.6678\n",
            "Epoch 18/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.9551 - acc: 0.6705 - val_loss: 0.9993 - val_acc: 0.6608\n",
            "Epoch 19/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.9478 - acc: 0.6721 - val_loss: 0.9852 - val_acc: 0.6642\n",
            "Epoch 20/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.9439 - acc: 0.6743 - val_loss: 0.9856 - val_acc: 0.6626\n",
            "Epoch 21/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.9363 - acc: 0.6766 - val_loss: 0.9822 - val_acc: 0.6632\n",
            "Epoch 22/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.9325 - acc: 0.6777 - val_loss: 0.9874 - val_acc: 0.6628\n",
            "Epoch 23/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.9302 - acc: 0.6788 - val_loss: 0.9838 - val_acc: 0.6672\n",
            "Epoch 24/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.9270 - acc: 0.6797 - val_loss: 0.9667 - val_acc: 0.6696\n",
            "Epoch 25/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.9210 - acc: 0.6825 - val_loss: 0.9733 - val_acc: 0.6646\n",
            "Epoch 26/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.9174 - acc: 0.6829 - val_loss: 0.9741 - val_acc: 0.6712\n",
            "Epoch 27/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.9150 - acc: 0.6844 - val_loss: 0.9738 - val_acc: 0.6680\n",
            "Epoch 28/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.9111 - acc: 0.6852 - val_loss: 0.9800 - val_acc: 0.6634\n",
            "Epoch 29/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.9077 - acc: 0.6863 - val_loss: 0.9669 - val_acc: 0.6678\n",
            "Epoch 30/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.9044 - acc: 0.6874 - val_loss: 0.9606 - val_acc: 0.6798\n",
            "Epoch 31/200\n",
            "352/352 [==============================] - 4s 12ms/step - loss: 0.9019 - acc: 0.6890 - val_loss: 0.9567 - val_acc: 0.6722\n",
            "Epoch 32/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.9013 - acc: 0.6896 - val_loss: 0.9585 - val_acc: 0.6694\n",
            "Epoch 33/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8947 - acc: 0.6906 - val_loss: 0.9552 - val_acc: 0.6742\n",
            "Epoch 34/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8949 - acc: 0.6901 - val_loss: 0.9590 - val_acc: 0.6718\n",
            "Epoch 35/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8904 - acc: 0.6919 - val_loss: 0.9563 - val_acc: 0.6754\n",
            "Epoch 36/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8860 - acc: 0.6937 - val_loss: 0.9599 - val_acc: 0.6728\n",
            "Epoch 37/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8853 - acc: 0.6941 - val_loss: 0.9585 - val_acc: 0.6748\n",
            "Epoch 38/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8847 - acc: 0.6940 - val_loss: 0.9518 - val_acc: 0.6764\n",
            "Epoch 39/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8790 - acc: 0.6964 - val_loss: 0.9471 - val_acc: 0.6780\n",
            "Epoch 40/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8805 - acc: 0.6953 - val_loss: 0.9543 - val_acc: 0.6806\n",
            "Epoch 41/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8781 - acc: 0.6959 - val_loss: 0.9456 - val_acc: 0.6776\n",
            "Epoch 42/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8748 - acc: 0.6975 - val_loss: 0.9373 - val_acc: 0.6770\n",
            "Epoch 43/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8729 - acc: 0.6986 - val_loss: 0.9397 - val_acc: 0.6760\n",
            "Epoch 44/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8696 - acc: 0.6995 - val_loss: 0.9415 - val_acc: 0.6780\n",
            "Epoch 45/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8686 - acc: 0.6991 - val_loss: 0.9489 - val_acc: 0.6786\n",
            "Epoch 46/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8655 - acc: 0.7002 - val_loss: 0.9619 - val_acc: 0.6702\n",
            "Epoch 47/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8643 - acc: 0.7005 - val_loss: 0.9406 - val_acc: 0.6776\n",
            "Epoch 48/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8614 - acc: 0.7012 - val_loss: 0.9453 - val_acc: 0.6798\n",
            "Epoch 49/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8629 - acc: 0.7015 - val_loss: 0.9353 - val_acc: 0.6840\n",
            "Epoch 50/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8605 - acc: 0.7019 - val_loss: 0.9498 - val_acc: 0.6728\n",
            "Epoch 51/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8570 - acc: 0.7030 - val_loss: 0.9409 - val_acc: 0.6776\n",
            "Epoch 52/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8562 - acc: 0.7034 - val_loss: 0.9357 - val_acc: 0.6790\n",
            "Epoch 53/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8535 - acc: 0.7034 - val_loss: 0.9500 - val_acc: 0.6718\n",
            "Epoch 54/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8536 - acc: 0.7039 - val_loss: 0.9411 - val_acc: 0.6800\n",
            "Epoch 55/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8515 - acc: 0.7036 - val_loss: 0.9327 - val_acc: 0.6792\n",
            "Epoch 56/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8503 - acc: 0.7049 - val_loss: 0.9279 - val_acc: 0.6804\n",
            "Epoch 57/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8483 - acc: 0.7057 - val_loss: 0.9637 - val_acc: 0.6696\n",
            "Epoch 58/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8470 - acc: 0.7054 - val_loss: 0.9473 - val_acc: 0.6764\n",
            "Epoch 59/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8464 - acc: 0.7059 - val_loss: 0.9363 - val_acc: 0.6838\n",
            "Epoch 60/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8481 - acc: 0.7057 - val_loss: 0.9452 - val_acc: 0.6744\n",
            "Epoch 61/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8437 - acc: 0.7071 - val_loss: 0.9424 - val_acc: 0.6794\n",
            "Epoch 62/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8428 - acc: 0.7079 - val_loss: 0.9335 - val_acc: 0.6822\n",
            "Epoch 63/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8443 - acc: 0.7071 - val_loss: 0.9216 - val_acc: 0.6780\n",
            "Epoch 64/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8410 - acc: 0.7076 - val_loss: 0.9500 - val_acc: 0.6760\n",
            "Epoch 65/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8402 - acc: 0.7089 - val_loss: 0.9311 - val_acc: 0.6812\n",
            "Epoch 66/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8414 - acc: 0.7075 - val_loss: 0.9274 - val_acc: 0.6816\n",
            "Epoch 67/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8394 - acc: 0.7081 - val_loss: 0.9262 - val_acc: 0.6792\n",
            "Epoch 68/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8375 - acc: 0.7095 - val_loss: 0.9394 - val_acc: 0.6772\n",
            "Epoch 69/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8359 - acc: 0.7095 - val_loss: 0.9307 - val_acc: 0.6810\n",
            "Epoch 70/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8367 - acc: 0.7093 - val_loss: 0.9206 - val_acc: 0.6852\n",
            "Epoch 71/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8340 - acc: 0.7104 - val_loss: 0.9212 - val_acc: 0.6814\n",
            "Epoch 72/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8343 - acc: 0.7101 - val_loss: 0.9349 - val_acc: 0.6766\n",
            "Epoch 73/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8346 - acc: 0.7100 - val_loss: 0.9297 - val_acc: 0.6766\n",
            "Epoch 74/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8306 - acc: 0.7112 - val_loss: 0.9295 - val_acc: 0.6806\n",
            "Epoch 75/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8314 - acc: 0.7114 - val_loss: 0.9411 - val_acc: 0.6752\n",
            "Epoch 76/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8320 - acc: 0.7109 - val_loss: 0.9265 - val_acc: 0.6732\n",
            "Epoch 77/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8282 - acc: 0.7132 - val_loss: 0.9262 - val_acc: 0.6822\n",
            "Epoch 78/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8293 - acc: 0.7122 - val_loss: 0.9275 - val_acc: 0.6824\n",
            "Epoch 79/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8258 - acc: 0.7138 - val_loss: 0.9430 - val_acc: 0.6778\n",
            "Epoch 80/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8285 - acc: 0.7124 - val_loss: 0.9349 - val_acc: 0.6752\n",
            "Epoch 81/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8270 - acc: 0.7127 - val_loss: 0.9170 - val_acc: 0.6832\n",
            "Epoch 82/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8269 - acc: 0.7134 - val_loss: 0.9189 - val_acc: 0.6840\n",
            "Epoch 83/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8264 - acc: 0.7128 - val_loss: 0.9206 - val_acc: 0.6804\n",
            "Epoch 84/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8236 - acc: 0.7135 - val_loss: 0.9333 - val_acc: 0.6758\n",
            "Epoch 85/200\n",
            "352/352 [==============================] - 4s 12ms/step - loss: 0.8238 - acc: 0.7143 - val_loss: 0.9448 - val_acc: 0.6754\n",
            "Epoch 86/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8257 - acc: 0.7132 - val_loss: 0.9202 - val_acc: 0.6834\n",
            "Epoch 87/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8207 - acc: 0.7152 - val_loss: 0.9171 - val_acc: 0.6868\n",
            "Epoch 88/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8234 - acc: 0.7139 - val_loss: 0.9213 - val_acc: 0.6818\n",
            "Epoch 89/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8211 - acc: 0.7144 - val_loss: 0.9226 - val_acc: 0.6822\n",
            "Epoch 90/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8212 - acc: 0.7153 - val_loss: 0.9301 - val_acc: 0.6824\n",
            "Epoch 91/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8200 - acc: 0.7154 - val_loss: 0.9193 - val_acc: 0.6832\n",
            "Epoch 92/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8204 - acc: 0.7153 - val_loss: 0.9351 - val_acc: 0.6822\n",
            "Epoch 93/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8176 - acc: 0.7167 - val_loss: 0.9164 - val_acc: 0.6868\n",
            "Epoch 94/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8189 - acc: 0.7154 - val_loss: 0.9133 - val_acc: 0.6822\n",
            "Epoch 95/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8184 - acc: 0.7161 - val_loss: 0.9346 - val_acc: 0.6808\n",
            "Epoch 96/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8168 - acc: 0.7168 - val_loss: 0.9305 - val_acc: 0.6780\n",
            "Epoch 97/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8166 - acc: 0.7172 - val_loss: 0.9212 - val_acc: 0.6832\n",
            "Epoch 98/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8151 - acc: 0.7169 - val_loss: 0.9321 - val_acc: 0.6834\n",
            "Epoch 99/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8185 - acc: 0.7154 - val_loss: 0.9411 - val_acc: 0.6824\n",
            "Epoch 100/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8158 - acc: 0.7163 - val_loss: 0.9262 - val_acc: 0.6840\n",
            "Epoch 101/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8157 - acc: 0.7166 - val_loss: 0.9137 - val_acc: 0.6898\n",
            "Epoch 102/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8143 - acc: 0.7176 - val_loss: 0.9300 - val_acc: 0.6776\n",
            "Epoch 103/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8119 - acc: 0.7173 - val_loss: 0.9197 - val_acc: 0.6842\n",
            "Epoch 104/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8127 - acc: 0.7173 - val_loss: 0.9236 - val_acc: 0.6806\n",
            "Epoch 105/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8129 - acc: 0.7174 - val_loss: 0.9312 - val_acc: 0.6852\n",
            "Epoch 106/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8118 - acc: 0.7175 - val_loss: 0.9238 - val_acc: 0.6818\n",
            "Epoch 107/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8103 - acc: 0.7194 - val_loss: 0.9217 - val_acc: 0.6828\n",
            "Epoch 108/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8109 - acc: 0.7189 - val_loss: 0.9193 - val_acc: 0.6854\n",
            "Epoch 109/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8128 - acc: 0.7176 - val_loss: 0.9265 - val_acc: 0.6830\n",
            "Epoch 110/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8096 - acc: 0.7187 - val_loss: 0.9225 - val_acc: 0.6846\n",
            "Epoch 111/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8116 - acc: 0.7182 - val_loss: 0.9116 - val_acc: 0.6902\n",
            "Epoch 112/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8091 - acc: 0.7186 - val_loss: 0.9304 - val_acc: 0.6802\n",
            "Epoch 113/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8088 - acc: 0.7194 - val_loss: 0.9290 - val_acc: 0.6788\n",
            "Epoch 114/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8100 - acc: 0.7185 - val_loss: 0.9135 - val_acc: 0.6846\n",
            "Epoch 115/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8065 - acc: 0.7199 - val_loss: 0.9247 - val_acc: 0.6860\n",
            "Epoch 116/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8104 - acc: 0.7182 - val_loss: 0.9363 - val_acc: 0.6790\n",
            "Epoch 117/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8067 - acc: 0.7198 - val_loss: 0.9237 - val_acc: 0.6842\n",
            "Epoch 118/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8064 - acc: 0.7200 - val_loss: 0.9154 - val_acc: 0.6882\n",
            "Epoch 119/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8063 - acc: 0.7193 - val_loss: 0.9273 - val_acc: 0.6830\n",
            "Epoch 120/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8075 - acc: 0.7193 - val_loss: 0.9258 - val_acc: 0.6844\n",
            "Epoch 121/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8047 - acc: 0.7207 - val_loss: 0.9166 - val_acc: 0.6830\n",
            "Epoch 122/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8056 - acc: 0.7208 - val_loss: 0.9259 - val_acc: 0.6824\n",
            "Epoch 123/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8030 - acc: 0.7212 - val_loss: 0.9137 - val_acc: 0.6884\n",
            "Epoch 124/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8034 - acc: 0.7213 - val_loss: 0.9149 - val_acc: 0.6870\n",
            "Epoch 125/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8074 - acc: 0.7195 - val_loss: 0.9131 - val_acc: 0.6922\n",
            "Epoch 126/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8046 - acc: 0.7202 - val_loss: 0.9247 - val_acc: 0.6870\n",
            "Epoch 127/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8040 - acc: 0.7205 - val_loss: 0.9323 - val_acc: 0.6748\n",
            "Epoch 128/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8040 - acc: 0.7203 - val_loss: 0.9200 - val_acc: 0.6878\n",
            "Epoch 129/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8051 - acc: 0.7196 - val_loss: 0.9242 - val_acc: 0.6818\n",
            "Epoch 130/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8023 - acc: 0.7204 - val_loss: 0.9313 - val_acc: 0.6812\n",
            "Epoch 131/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8037 - acc: 0.7204 - val_loss: 0.9120 - val_acc: 0.6914\n",
            "Epoch 132/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8001 - acc: 0.7209 - val_loss: 0.9214 - val_acc: 0.6840\n",
            "Epoch 133/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8016 - acc: 0.7213 - val_loss: 0.9171 - val_acc: 0.6850\n",
            "Epoch 134/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8013 - acc: 0.7213 - val_loss: 0.9258 - val_acc: 0.6756\n",
            "Epoch 135/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8021 - acc: 0.7208 - val_loss: 0.9226 - val_acc: 0.6868\n",
            "Epoch 136/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8031 - acc: 0.7200 - val_loss: 0.9354 - val_acc: 0.6870\n",
            "Epoch 137/200\n",
            "352/352 [==============================] - 3s 10ms/step - loss: 0.7987 - acc: 0.7223 - val_loss: 0.9136 - val_acc: 0.6878\n",
            "Epoch 138/200\n",
            "352/352 [==============================] - 4s 11ms/step - loss: 0.7997 - acc: 0.7214 - val_loss: 0.9212 - val_acc: 0.6876\n",
            "Epoch 139/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8007 - acc: 0.7215 - val_loss: 0.9210 - val_acc: 0.6870\n",
            "Epoch 140/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7972 - acc: 0.7224 - val_loss: 0.9152 - val_acc: 0.6852\n",
            "Epoch 141/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7989 - acc: 0.7211 - val_loss: 0.9192 - val_acc: 0.6832\n",
            "Epoch 142/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7983 - acc: 0.7221 - val_loss: 0.9170 - val_acc: 0.6882\n",
            "Epoch 143/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.8002 - acc: 0.7214 - val_loss: 0.9173 - val_acc: 0.6926\n",
            "Epoch 144/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7978 - acc: 0.7226 - val_loss: 0.9212 - val_acc: 0.6822\n",
            "Epoch 145/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7983 - acc: 0.7223 - val_loss: 0.9312 - val_acc: 0.6858\n",
            "Epoch 146/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7983 - acc: 0.7218 - val_loss: 0.9313 - val_acc: 0.6784\n",
            "Epoch 147/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7962 - acc: 0.7226 - val_loss: 0.9123 - val_acc: 0.6916\n",
            "Epoch 148/200\n",
            "352/352 [==============================] - 3s 10ms/step - loss: 0.7955 - acc: 0.7225 - val_loss: 0.9345 - val_acc: 0.6856\n",
            "Epoch 149/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7959 - acc: 0.7233 - val_loss: 0.9119 - val_acc: 0.6864\n",
            "Epoch 150/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7956 - acc: 0.7231 - val_loss: 0.9118 - val_acc: 0.6910\n",
            "Epoch 151/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7952 - acc: 0.7233 - val_loss: 0.9142 - val_acc: 0.6862\n",
            "Epoch 152/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7946 - acc: 0.7231 - val_loss: 0.9144 - val_acc: 0.6840\n",
            "Epoch 153/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7963 - acc: 0.7227 - val_loss: 0.9169 - val_acc: 0.6852\n",
            "Epoch 154/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7944 - acc: 0.7237 - val_loss: 0.9234 - val_acc: 0.6880\n",
            "Epoch 155/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7949 - acc: 0.7236 - val_loss: 0.9188 - val_acc: 0.6858\n",
            "Epoch 156/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7940 - acc: 0.7231 - val_loss: 0.9140 - val_acc: 0.6856\n",
            "Epoch 157/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7948 - acc: 0.7230 - val_loss: 0.9111 - val_acc: 0.6870\n",
            "Epoch 158/200\n",
            "352/352 [==============================] - 3s 10ms/step - loss: 0.7936 - acc: 0.7231 - val_loss: 0.9251 - val_acc: 0.6862\n",
            "Epoch 159/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7948 - acc: 0.7226 - val_loss: 0.9270 - val_acc: 0.6866\n",
            "Epoch 160/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7928 - acc: 0.7237 - val_loss: 0.9141 - val_acc: 0.6924\n",
            "Epoch 161/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7917 - acc: 0.7245 - val_loss: 0.9191 - val_acc: 0.6906\n",
            "Epoch 162/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7937 - acc: 0.7239 - val_loss: 0.9175 - val_acc: 0.6870\n",
            "Epoch 163/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7919 - acc: 0.7237 - val_loss: 0.9228 - val_acc: 0.6874\n",
            "Epoch 164/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7923 - acc: 0.7240 - val_loss: 0.9179 - val_acc: 0.6886\n",
            "Epoch 165/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7922 - acc: 0.7248 - val_loss: 0.9178 - val_acc: 0.6910\n",
            "Epoch 166/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7916 - acc: 0.7239 - val_loss: 0.9133 - val_acc: 0.6886\n",
            "Epoch 167/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7910 - acc: 0.7246 - val_loss: 0.9211 - val_acc: 0.6850\n",
            "Epoch 168/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7909 - acc: 0.7247 - val_loss: 0.9139 - val_acc: 0.6854\n",
            "Epoch 169/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7898 - acc: 0.7249 - val_loss: 0.9126 - val_acc: 0.6894\n",
            "Epoch 170/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7898 - acc: 0.7248 - val_loss: 0.9293 - val_acc: 0.6848\n",
            "Epoch 171/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7894 - acc: 0.7256 - val_loss: 0.9356 - val_acc: 0.6824\n",
            "Epoch 172/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7911 - acc: 0.7250 - val_loss: 0.9203 - val_acc: 0.6904\n",
            "Epoch 173/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7882 - acc: 0.7263 - val_loss: 0.9207 - val_acc: 0.6878\n",
            "Epoch 174/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7903 - acc: 0.7252 - val_loss: 0.9160 - val_acc: 0.6872\n",
            "Epoch 175/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7881 - acc: 0.7249 - val_loss: 0.9155 - val_acc: 0.6860\n",
            "Epoch 176/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7901 - acc: 0.7250 - val_loss: 0.9192 - val_acc: 0.6848\n",
            "Epoch 177/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7897 - acc: 0.7256 - val_loss: 0.9173 - val_acc: 0.6888\n",
            "Epoch 178/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7884 - acc: 0.7253 - val_loss: 0.9187 - val_acc: 0.6906\n",
            "Epoch 179/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7865 - acc: 0.7261 - val_loss: 0.9372 - val_acc: 0.6840\n",
            "Epoch 180/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7886 - acc: 0.7256 - val_loss: 0.9203 - val_acc: 0.6876\n",
            "Epoch 181/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7873 - acc: 0.7257 - val_loss: 0.9186 - val_acc: 0.6838\n",
            "Epoch 182/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7897 - acc: 0.7251 - val_loss: 0.9218 - val_acc: 0.6874\n",
            "Epoch 183/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7881 - acc: 0.7250 - val_loss: 0.9157 - val_acc: 0.6882\n",
            "Epoch 184/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7882 - acc: 0.7254 - val_loss: 0.9253 - val_acc: 0.6874\n",
            "Epoch 185/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7871 - acc: 0.7258 - val_loss: 0.9188 - val_acc: 0.6854\n",
            "Epoch 186/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7868 - acc: 0.7262 - val_loss: 0.9238 - val_acc: 0.6874\n",
            "Epoch 187/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7875 - acc: 0.7259 - val_loss: 0.9244 - val_acc: 0.6844\n",
            "Epoch 188/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7859 - acc: 0.7262 - val_loss: 0.9183 - val_acc: 0.6840\n",
            "Epoch 189/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7865 - acc: 0.7253 - val_loss: 0.9321 - val_acc: 0.6846\n",
            "Epoch 190/200\n",
            "352/352 [==============================] - 4s 12ms/step - loss: 0.7851 - acc: 0.7257 - val_loss: 0.9143 - val_acc: 0.6898\n",
            "Epoch 191/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7837 - acc: 0.7277 - val_loss: 0.9280 - val_acc: 0.6818\n",
            "Epoch 192/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7854 - acc: 0.7263 - val_loss: 0.9302 - val_acc: 0.6828\n",
            "Epoch 193/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7869 - acc: 0.7256 - val_loss: 0.9152 - val_acc: 0.6862\n",
            "Epoch 194/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7853 - acc: 0.7260 - val_loss: 0.9201 - val_acc: 0.6842\n",
            "Epoch 195/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7846 - acc: 0.7270 - val_loss: 0.9224 - val_acc: 0.6846\n",
            "Epoch 196/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7834 - acc: 0.7265 - val_loss: 0.9210 - val_acc: 0.6864\n",
            "Epoch 197/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7853 - acc: 0.7268 - val_loss: 0.9127 - val_acc: 0.6870\n",
            "Epoch 198/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7850 - acc: 0.7268 - val_loss: 0.9160 - val_acc: 0.6862\n",
            "Epoch 199/200\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.7853 - acc: 0.7261 - val_loss: 0.9304 - val_acc: 0.6798\n",
            "Epoch 200/200\n",
            "352/352 [==============================] - 4s 10ms/step - loss: 0.7864 - acc: 0.7261 - val_loss: 0.9205 - val_acc: 0.6832\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting both the models"
      ],
      "metadata": {
        "id": "HedmA0p6hiDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(1,201),history_1.history['val_acc'], label ='Normal Data Model')\n",
        "plt.plot(range(1,201),history_2.history['val_acc'], label ='Augmented Data Model')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Validation accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "G8gIrBdGlbdm",
        "outputId": "299655e0-38df-49d0-c718-464bdf5c6d5e"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2PmEI8kDifdx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}